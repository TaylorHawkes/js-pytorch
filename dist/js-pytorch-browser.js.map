{"version":3,"file":"js-pytorch-browser.js","sources":["../src/utils.ts","../src/tensor.ts","../src/layers.ts","../src/optim.ts","../src/index.ts"],"sourcesContent":["import { Tensor } from \"./tensor\";\nimport NestedArray from \"./types/utils.types\";\n\n/**\n * Recursively gets the shape (length of every dimension) of the Tensor.\n * @param {object} data - Iterable containing the data to be stored in the Tensor.\n * @param {object} shape - Length of every dimension of the Tensor.\n * @returns {object} Length of every dimension of the Tensor.\n */\n\nexport function getShape(\n  data: NestedArray<number> | number,\n  shape: Array<number> = []\n): Array<number> {\n  // Edge case empty array:\n  if (data instanceof Array && data.length === 0) {\n    return [0];\n  }\n\n  // Base case:\n  if (typeof data === \"number\") {\n    // Return [1] for integers:\n    if (JSON.stringify(shape) === \"[]\") {\n      return [1];\n    }\n    // Return shape for objects:\n    return shape;\n  }\n\n  if (typeof data[0] === \"number\" && Array.isArray(data)) {\n    for (const element of data) {\n      if (typeof element != \"number\") {\n        throw new Error(\"The requested array has an inhomogeneous shape.\");\n      }\n    }\n    // Return shape for objects:\n    shape.push(data.length);\n    return shape;\n  }\n\n  if (Array.isArray(data[0])) {\n    let elementLength = data[0].length;\n    // Iterate over elements in dimention to check if all have the same length (homogenous shape):\n    for (const element of data) {\n      // Throw error if the element is not a number or another iterable:\n      if (typeof element != \"object\" && typeof element != \"number\") {\n        throw new Error(\"TypeError: the input data is not a number.\");\n      } else if (Array.isArray(element) && elementLength != element.length) {\n        // Throw error if the shape is inhomogenous:\n        throw new Error(\"The requested array has an inhomogeneous shape.\");\n      } else if (Array.isArray(element)) {\n        elementLength = element.length;\n      }\n    }\n    shape.push(data.length);\n  }\n  return getShape(data[0], shape);\n}\n\n/**\n * Assures that the returned iterable is a vanilla JavaScript Array object:\n * @param {object} a - Any numeric iterable or number.\n * @returns {object} Original iterable in an Array format.\n */\nexport function assureArray(a: Tensor | Array<number> | number): Array<number> {\n  if (Array.isArray(a)) {\n    return a;\n  } else if (typeof a === \"number\") {\n    return [a];\n  } else if (a === null) {\n    return a;\n  }\n  return a._data;\n}\n\n/**\n * Assures that the returned iterable is of a vanilla JavaScript data type (num of Array object):\n * @param {object} a - Any numeric iterable or number.\n * @returns {object} Original data in a vanilla format.\n */\nexport function getData(\n  a: Tensor | Array<number> | number\n): Array<number> | number {\n  if (Array.isArray(a)) {\n    return a;\n  }\n  if (typeof a === \"number\") {\n    return a;\n  }\n  return a._data;\n}\n","import { getShape, getData, assureArray } from \"./utils\";\n\n// <<< Tensor class, holds n-dimensional tensors, and multiple useful methods >>> //\n\nexport class Tensor {\n  public requires_grad: boolean = false;\n  public _data: Array<any>;\n  public shape: Array<any>;\n  public _grad!: Tensor;\n  public children: Array<any>;\n  public parents: Array<any>;\n  public operation: any;\n  public visited: boolean = false;\n  public m!: Tensor;\n  public v!: Tensor;\n  public device: string;\n  public forwardKernel: any;\n  public backwardKernelA: any;\n  public backwardKernelB: any;\n  public batch_size: number | null;\n  public gpu: any;\n  public warned: boolean;\n\n  /**\n   * Creates new instance of the Tensor class.\n   * @param {object} data - Iterable containing the data to be stored in the Tensor.\n   * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n   * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n   */\n  constructor(\n    data: Array<any> | number,\n    requires_grad = false,\n    device = \"cpu\"\n  ) {\n    if (typeof data === \"object\") {\n      this._data = data;\n    } else if (typeof data === \"number\") {\n      this._data = [data];\n    } else {\n      throw Error('Your argument \"data\" is not a number or an iterable.');\n    }\n    this.shape = getShape(data);\n    this.device = device;\n    this.requires_grad = requires_grad;\n    this.forwardKernel = null;\n    this.batch_size = null;\n    this.gpu = null;\n    this.warned = false;\n\n    // Initialize momentum and velocity cumulatives for every parameter:\n    if (this.requires_grad) {\n      this._grad = zeros(this.shape);\n    }\n\n    // Graph connections:\n    this.children = [];\n    this.parents = [];\n    this.operation = null;\n    this.visited = false;\n  }\n\n  /**\n   * Returns the data in the Tensor.\n   */\n  get data(): Array<any> {\n    return this._data;\n  }\n\n  /**\n   * Returns the data's length'.\n   */\n  get length() {\n    return this._data.length;\n  }\n\n  /**\n   * Returns the number of dimensions in the Tensor.\n   */\n  get ndims() {\n    return this.shape.length;\n  }\n\n  /**\n   * Returns the tensor's gradients.\n   */\n  get grad() {\n    return this._grad?.data;\n  }\n\n  /**\n   * Performs backward pass from THIS tensor backwards.\n   * It fills every tensor that originated this one and that has requires_grad=true's gradients to their gradients relative to THIS tensor.\n   */\n  backward(grad: Tensor | null = null, child: Tensor | null = null) {\n    // Guarantee that this tensor requires grad:\n    if (!this.requires_grad) {\n      throw new Error(\"this tensor has requires_grad set to False\");\n    }\n\n    // If this is the root tensor, grad is just ones,\n    // and we remove all children from this point on from the Graph:\n    if (grad === null) {\n      grad = ones(this.shape);\n      this.children = [];\n    }\n\n    // Add upstream gradient to this._grad:\n    this._grad = new Tensor(_add(this._grad?.data, grad.data));\n\n    if (child != null) {\n      const idx = this.children.indexOf(child);\n      this.children.splice(idx, 1);\n    }\n\n    // If this Tensor is the product of an Operation:\n    if (this.operation != null) {\n      // When all the children have been visited, propagate further back:\n      if (this.children.length === 0) {\n        this.operation.backward(this._grad, this);\n      }\n    }\n  }\n\n  /**\n   * Sends this Tensor to the provided device.\n   * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n   * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n   * @param {string} device - gpu or cpu: device to store Tensor.\n   */\n  to(device: string) {\n    this.device = device;\n  }\n\n  /**\n   * Reset this Tensor's gradients to zero.\n   */\n  zero_grad() {\n    this._grad = zeros(this.shape);\n    this.children = [];\n    this.parents = [];\n    this.operation = null;\n    if (this.m instanceof Tensor && this.v instanceof Tensor) {\n      this.m.zero_grad_graph();\n      this.v.zero_grad_graph();\n    }\n  }\n\n  /**\n   * Reset the gradients of this Tensor, and of all of the Tensors that led to it.\n   */\n  zero_grad_graph() {\n    this.zero_grad();\n    if (this.operation != null) {\n      for (const parent of this.parents) {\n        parent.zero_grad_graph();\n        parent.parents = [];\n      }\n      this.operation = null;\n      this.parents = [];\n      this.children = [];\n    }\n  }\n\n  /**\n   * Turns the data in the Tensor into a javascript list object.\n   */\n  tolist() {\n    return this._data;\n  }\n\n  /**\n   * Gets the sum of the Tensor over a specified dimension.\n   * @param {number} dim - Dimension to sum over.\n   * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  sum(dim = -1, keepdims = false) {\n    const operation = new Sum();\n    return operation.forward(this, dim, keepdims);\n  }\n\n  /**\n   * Gets the mean of the Tensor over a specified dimension.\n   * @param {number} dim - Dimension to get mean over.\n   * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  mean(dim = -1, keepdims = false) {\n    const operation = new Mean();\n    return operation.forward(this, dim, keepdims);\n  }\n\n  /**\n   * Gets the variance of the Tensor over a specified dimension.\n   * @param {number} dim - Dimension to get variance over.\n   * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  variance(dim = -1, keepdims = false) {\n    const operation = new Variance();\n    return operation.forward(this, dim, keepdims);\n  }\n\n  /**\n   * Add integer or other Tensor to this Tensor.\n   * @param {any} other - Tensor or integer to be added to this Tensor.\n   * @returns {object} New tensor.\n   */\n  add(other: Tensor | number): Tensor {\n    const operation = new Add();\n    return operation.forward(this, other);\n  }\n\n  /**\n   * Subtract integer or other Tensor from this Tensor.\n   * @param {any} other - Tensor or integer to be subtracted from this Tensor.\n   * @returns {object} New tensor.\n   */\n  sub(other: Tensor | number): Tensor {\n    if (typeof other === \"number\") {\n      return this.add(-other);\n    } else if (other instanceof Tensor) {\n      return this.add(other.neg());\n    } else {\n      throw Error('Argument \"other\" is not a Tensor or a number.');\n    }\n  }\n\n  /**\n   * Get element-wise opposite of given tensor ( every element * (-1) )\n   * @returns {object} New tensor.\n   */\n  neg() {\n    const operation = new Neg();\n    return operation.forward(this);\n  }\n\n  /**\n   * Multiply this Tensor by integer or other Tensor.\n   * @param {any} other - Tensor or integer to multiply this Tensor by.\n   * @returns {object} New tensor.\n   */\n  mul(other: Tensor | number): Tensor {\n    const operation = new Mul();\n    return operation.forward(this, other);\n  }\n\n  /**\n   * Divide this Tensor by integer or other Tensor.\n   * @param {Tensor | number} other - Tensor or integer to divide this Tensor by.\n   * @returns {Tensor} New tensor.\n   */\n  div(other: Tensor | number): Tensor {\n    const operation = new Div();\n    return operation.forward(this, other);\n  }\n\n  /**\n   * Multiply this Tensor by integer or other Tensor.\n   * @param {Tensor | number} other - Tensor or integer to multiply this Tensor by.\n   * @returns {Tensor} New tensor.\n   */\n  matmul(other: Tensor): Tensor {\n    const operation = new MatMul();\n    let device;\n    if (this.device === \"gpu\" || other.device === \"gpu\") {\n      device = \"gpu\";\n    } else {\n      device = \"cpu\";\n    }\n    // On first iteration, create CPU or GPU kernel for matmul:\n    if (other.forwardKernel === null || other.batch_size != this.shape.at(-2)) {\n      if (device === \"gpu\") {\n        // Get GPU from GPU.js:\n        const { GPU } = require(\"@eduardoleao052/gpu\");\n        // If the batch size changed, warn user and update the batch size:\n        if (other.batch_size != null) {\n          other.batch_size = other.shape.at(-2);\n          if (other.warned === false) {\n            console.warn(\n              \"Testing batch size different from training batch size. JS-PyTorch recreating GPU Kernel (Less efficient)\"\n            );\n            other.warned = true;\n          }\n        }\n        other.gpu = new GPU();\n        // Define Kernel function for matmul:\n        const kernelFunc = function (\n          this: any,\n          a: number[][],\n          b: number[][],\n          len: number\n        ): number {\n          let sum = 0;\n          for (let i = 0; i < len; i++) {\n            sum += a[this.thread.y][i] * b[i][this.thread.x];\n          }\n          return sum;\n        };\n        // Create and store the GPU kernels:\n        other.forwardKernel = other.gpu\n          .createKernel(kernelFunc, { loopMaxIterations: other.shape.at(-2) })\n          .setOutput([other.shape.at(-1), this.shape.at(-2)]);\n        other.backwardKernelA = other.gpu\n          .createKernel(kernelFunc, { loopMaxIterations: other.shape.at(-1) })\n          .setOutput([this.shape.at(-1), this.shape.at(-2)]);\n        other.backwardKernelB = other.gpu\n          .createKernel(kernelFunc, { loopMaxIterations: this.shape.at(-2) })\n          .setOutput([other.shape.at(-1), other.shape.at(-2)]);\n      } else {\n        // Build the CPU kernel:\n        const kernelFunc = function (\n          a: number[][],\n          b: number[][],\n          len: number\n        ) {\n          const out = Array(a.length)\n            .fill(0)\n            .map(() => Array(b[0].length).fill(0));\n          for (let i = 0; i < a.length; i++) {\n            for (let j = 0; j < b[0].length; j++) {\n              let currentIndex = 0;\n              for (let k = 0; k < len; k++) {\n                currentIndex += a[i][k] * b[k][j];\n              }\n              out[i][j] = currentIndex;\n            }\n          }\n          return out;\n        };\n        // Store the CPU kernels:\n        other.forwardKernel = kernelFunc;\n        other.backwardKernelA = kernelFunc;\n        other.backwardKernelB = kernelFunc;\n      }\n    }\n    // Store the batch size. If the batch size changes, we will create a new GPU kernel:\n    other.batch_size = this.shape.at(-2);\n    return operation.forward(this, other);\n  }\n\n  /**\n   * Get tensor to element-wise power of n.\n   * @param {number} n - Exponent.\n   * @returns {object} New tensor.\n   */\n  pow(n: number): Tensor {\n    const operation = new Pow();\n    return operation.forward(this, n);\n  }\n\n  /**\n   * Get element-wise square root of given tensor.\n   * @returns {object} New tensor.\n   */\n  sqrt() {\n    const operation = new Sqrt();\n    return operation.forward(this);\n  }\n\n  /**\n   * Get element-wise exponentiation of given tensor ( e^(every element) )\n   * @returns {object} New tensor.\n   */\n  exp() {\n    const operation = new Exp();\n    return operation.forward(this);\n  }\n\n  /**\n   * Get element-wise natural log of given tensor ( ln(every element) )\n   * @returns {object} New tensor.\n   */\n  log() {\n    const operation = new Log();\n    return operation.forward(this);\n  }\n\n  /**\n   * Transpose the tensor along two consecutive dimensions:\n   * @param {number} dim1 - First dimension.\n   * @param {number} dim2 - Second dimension.\n   * @returns {object} New tensor.\n   */\n  transpose(dim1: number, dim2: number): Tensor {\n    const operation = new Transpose();\n    return operation.forward(this, dim1, dim2);\n  }\n\n  /**\n   * In a tensor, returns a list of elements in [index1], or [index1][index2];\n   * @param {object} index1 - List containing indexes to extract data from in first dimension.\n   * @param {object} index2 - List containing indexes to extract data from in second dimension [OPTIONAL].\n   * @returns {object} New tensor.\n   * @example\n   * let a = tensor([[1,1,2,3],\n   *                 [6,7,8,9]])\n   *\n   * // Returns tensor([2,6,9]):\n   * a.at([0,1,1], [2,0,3])\n   *\n   * // Returns tensor([[1,1,2,3],\n   *                    [6,7,8,9],\n   *                    [1,1,2,3]])\n   * a.at([0,1,0])\n   */\n  at(index1: Tensor | Array<any>, index2?: Tensor | Array<any>): Tensor {\n    const operation = new At();\n    return operation.forward(this, index1, index2);\n  }\n\n  /**\n   * Where the \"condition\" function returns True in \"mask\" Tensor, the \"value\" will fill the \"this\" Tensor.\n   * @param {Tensor} mask - \"condition\" will be applied in this tensor element-wise.\n   * @param {function} condition - Function that returns True or False element-wise.\n   * @param {number} value - Value to fill Tensor when condition is met.\n   * @returns {object} New tensor.\n   * @example\n   * let a = tensor([[1,5,2,3],\n   *                 [6,7,2,9]])\n   *\n   * // Returns tensor([[1,0,2,3],\n   * //                 [0,0,2,0]])\n   * a.masked_fill(mask, (el) => {return el > 3}, 0)\n   */\n  masked_fill(\n    mask: Tensor,\n    condition: (someArg: number) => boolean,\n    value: number\n  ) {\n    const operation = new MaskedFill();\n    return operation.forward(this, mask, condition, value);\n  }\n\n  /**\n   * Reshape the tensor into the new shape:\n   * @param {object} shape - New tensor's shape.\n   * @returns {object} New tensor.\n   */\n  reshape(shape: Array<number>) {\n    const operation = new Reshape();\n    return operation.forward(this, shape);\n  }\n}\n\n// <<< Parameter class, tensor that always tracks gradients >>> //\n\nexport class Parameter extends Tensor {\n  /**\n   * Creates new Parameter (an instance of the Tensor class that always tracks gradients).\n   * @param {object} data - Iterable containing the data to be stored in the Tensor.\n   */\n  constructor(data: Array<any> | number) {\n    super(data, true);\n  }\n}\n\n// <<< Basic Operations >>> //\n\nexport class Add {\n  cache: any;\n\n  /**\n   * Add tensors or tensor and integers.\n   * @param {any} a - First tensor or integer.\n   * @param {any} b - Second tensor or integer.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor | number | number, b: Tensor | number | number): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, b];\n\n    const aData = getData(a);\n    const bData = getData(b);\n\n    // Call recursive function:\n    const z = new Tensor(\n      _add(aData, bData), // data;\n      requiresGrad(a) || requiresGrad(b) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (a instanceof Tensor && requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n    }\n    if (b instanceof Tensor && requiresGrad(b)) {\n      z.parents.push(b);\n      b.children.push(z);\n    }\n    z.operation = this;\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, b] = this.cache;\n\n    // Find gradients relative to \"a\", and pass it downstream:\n    if (requiresGrad(a)) {\n      let da = dz;\n      // Rescale gradient to have the same shape as \"a\":\n      da = broadcast(da, a);\n      a.backward(da, z);\n    }\n\n    // Find gradients relative to \"b\", and pass it downstream:\n    if (requiresGrad(b)) {\n      let db = dz;\n      // Rescale gradient to have the same shape as \"b\":\n      db = broadcast(db, b);\n      b.backward(db, z);\n    }\n  }\n}\n\nexport class Neg {\n  cache: any;\n\n  /**\n   * Get element-wise opposite of given tensor ( every element * (-1) )\n   * @param {object} a - Tensor to be multiplied by -1.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call recursive function:\n    const z = new Tensor(\n      _neg(a._data), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    if (requiresGrad(a)) {\n      const da = neg(dz);\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Mul {\n  cache: any;\n\n  /**\n   * Perform element-wise multiplication between Tensors and integers or other Tensors.\n   * @param {any} a - First tensor or integer.\n   * @param {any} b - Second tensor or integer.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor | number, b: Tensor | number): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, b];\n\n    const aData = getData(a);\n    const bData = getData(b);\n\n    // Call recursive function:\n    const z = new Tensor(\n      _mul(aData, bData), // data;\n      requiresGrad(a) || requiresGrad(b) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (a instanceof Tensor && requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n    }\n    if (b instanceof Tensor && requiresGrad(b)) {\n      z.parents.push(b);\n      b.children.push(z);\n    }\n    z.operation = this;\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, b] = this.cache;\n\n    // Find gradients relative to \"a\", and pass it downstream:\n    if (requiresGrad(a)) {\n      let da = new Tensor(_mul(dz.data, getData(b)));\n      // Rescale gradient to have the same shape as \"a\":\n      da = broadcast(da, a);\n      a.backward(da, z);\n    }\n\n    // Find gradients relative to \"b\", and pass it downstream:\n    if (requiresGrad(b)) {\n      let db = new Tensor(_mul(dz.data, getData(a)));\n      // Rescale gradient to have the same shape as \"b\":\n      db = broadcast(db, b);\n      b.backward(db, z);\n    }\n  }\n}\n\nexport class Div {\n  cache: any;\n\n  /**\n   * Perform element-wise division between Tensors and integers or other Tensors.\n   * @param {any} a - First tensor or integer.\n   * @param {any} b - Second tensor or integer.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor, b: Tensor | number): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, b];\n\n    const aData = getData(a);\n    const bData = getData(b);\n\n    // Call recursive function:\n    const z = new Tensor(\n      _div(aData, bData), // data;\n      requiresGrad(a) || requiresGrad(b) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (a instanceof Tensor && requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n    }\n    if (b instanceof Tensor && requiresGrad(b)) {\n      z.parents.push(b);\n      b.children.push(z);\n    }\n    z.operation = this;\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, b] = this.cache;\n\n    // Find gradients relative to \"a\", and pass it downstream:\n    if (requiresGrad(a)) {\n      // d/da(a/b) = (1/b), apply chain rule:\n      let da = new Tensor(_mul(dz.data, _div(1, getData(b))));\n\n      // Rescale gradient to have the same shape as \"a\":\n      da = broadcast(da, a);\n\n      a.backward(da, z);\n    }\n\n    // Find gradients relative to \"b\", and pass it downstream:\n    if (requiresGrad(b)) {\n      // d/db(a/b) = -(a/b^2), apply chain rule:\n      let db = new Tensor(\n        _mul(dz.data, _neg(_div(getData(a), _pow(getData(b), 2))))\n      );\n      // Rescale gradient to have the same shape as \"b\":\n      db = broadcast(db, b);\n\n      b.backward(db, z);\n    }\n  }\n}\n\nclass MatMul {\n  cache: any;\n  kernelFunc: any;\n  thread: any;\n  forward(a: Tensor, b: Tensor) {\n    this.cache = [a, b];\n    let aData = a.data;\n    let bData = b.data;\n\n    if (a.shape.length < b.shape.length) {\n      aData = broadcastUp(aData, bData);\n    } else {\n      bData = broadcastUp(bData, aData);\n    }\n    const z = new Tensor(\n      _matmul(aData, bData, b.forwardKernel),\n      // data;\n      requiresGrad(a) || requiresGrad(b)\n      // requires_grad;\n    );\n    if (a instanceof Tensor && requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n    }\n    if (b instanceof Tensor && requiresGrad(b)) {\n      z.parents.push(b);\n      b.children.push(z);\n    }\n    z.operation = this;\n    return z;\n  }\n  backward(dz: Tensor, z: Tensor) {\n    const [a, b] = this.cache;\n    if (requiresGrad(a)) {\n      const dzData = dz.data;\n      let b_T = _transpose(b.data, b.ndims - 2);\n      b_T = broadcastUp(b_T, dzData);\n      let da = new Tensor(_matmul(dzData, b_T, b.backwardKernelA));\n      da = broadcast(da, a);\n      a.backward(da, z);\n    }\n    if (requiresGrad(b)) {\n      const dzData = dz.data;\n      let a_T = _transpose(a.data, a.ndims - 2);\n      a_T = broadcastUp(a_T, dzData);\n      let db = new Tensor(_matmul(a_T, dzData, b.backwardKernelB));\n      db = broadcast(db, b);\n      b.backward(db, z);\n    }\n  }\n}\n\nexport class Pow {\n  cache: any;\n\n  /**\n   * Get tensor to element-wise power of n.\n   * @param {object} a - Tensor to be elevated to the power of n.\n   * @param {number} n - Exponent.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor, n: number): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call recursive function:\n    const z = new Tensor(\n      _pow(getData(a), n), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // d/da(e^a) = e^a, apply the chain rule to the derivative of e^a:\n      const da = new Tensor(_mul(2, _mul(a.data, dz.data)));\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Sqrt {\n  cache: any;\n\n  /**\n   * Get element-wise square root of given tensor\n   * @param {object} a - Tensor to be square rooted.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call recursive function:\n    const z = new Tensor(\n      _sqrt(a._data), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // d/da(sqrt(a)) = (1/2) *  (1/sqrt(a)), apply the chain rule to the derivative of e^a:\n      const da = new Tensor(\n        _mul(_mul(_div(1, 2), _div(1, _sqrt(a.data))), dz.data)\n      );\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Exp {\n  cache: any;\n  /**\n   * Get element-wise exponentiation of given tensor ( e^(every element) )\n   * @param {object} a - Tensor to be exponentiated.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call recursive function:\n    const z = new Tensor(\n      _exp(a._data), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // d/da(e^a) = e^a, apply the chain rule to the derivative of e^a:\n      const da = new Tensor(_mul(_exp(a.data), dz.data));\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Log {\n  cache: any;\n\n  /**\n   * Get element-wise natural log of given tensor ( ln(every element) )\n   * @param {object} a - Tensor we will take the log of.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call recursive function:\n    const z = new Tensor(\n      _log(a._data), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // d/da(ln(a)) = (1/a), apply the chain rule to the derivative of the natural log:\n      const da = new Tensor(_mul(_div(1, a.data), dz.data));\n\n      a.backward(da, z);\n    }\n  }\n}\n\n// <<< Tensor Statistics >>> //\n\nexport class Sum {\n  cache: any;\n\n  /**\n   * Gets the sum of a Tensor over a specified dimension.\n   * @param {Tensor} a - Tensor to sum.\n   * @param {dim} dim - Dimension to sum over.\n   * @param {keepdims} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  forward(a: Tensor, dim: number, keepdims = false): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, dim, keepdims];\n\n    // Account for negative dimension index:\n    if (dim < 0) {\n      dim = a.shape.length + dim;\n    }\n    // Return error if dimension is out of bounds:\n    if (dim >= a.shape.length) {\n      throw Error(\"Dimension larger than array.\");\n    }\n    // Create output tensor:\n    const z = new Tensor(\n      _sum(a._data, dim, keepdims), // New data.\n      requiresGrad(a) // requires_grad.\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, dim, keepdims] = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      if (keepdims) {\n        dz = dz.sum(dim);\n      }\n\n      const da = broadcast(dz, a);\n\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Mean {\n  cache: any;\n\n  /**\n   * Gets the mean of a Tensor over a specified dimension.\n   * @param {Tensor} a - Tensor to get mean from.\n   * @param {dim} dim - Dimension to get mean over.\n   * @param {keepdims} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  forward(a: Tensor, dim: number, keepdims = false): Tensor {\n    // Account for negative dimension index:\n    if (dim < 0) {\n      dim = a.shape.length + dim;\n    }\n    // Return error if dimension is out of bounds:\n    if (dim >= a.shape.length) {\n      throw Error(\"Dimension larger than array.\");\n    }\n\n    // Build cache to use in backward step:\n    this.cache = [a, dim];\n\n    // Create output tensor:\n    const z = new Tensor(\n      _mean(a._data, dim, keepdims), // New data.\n      requiresGrad(a) // keep_dims.\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, dim] = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // Backprop through mean:\n      let da = new Tensor(_div(dz.data, a.shape[dim]));\n      // Expand upstream gradients to the shape of \"a\":\n      da = broadcast(da, a);\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Variance {\n  cache: any;\n  /**\n   * Gets the variance of a Tensor over a specified dimension.\n   * @param {Tensor} a - Tensor to get variance of.\n   * @param {dim} dim - Dimension to get variance over.\n   * @param {keepdims} keepdims - Whether to keep dimensions of original tensor.\n   * @returns {Tensor} - Final tensor.\n   */\n  forward(a: Tensor, dim: number, keepdims = false): Tensor {\n    // Account for negative dimension index:\n    if (dim < 0) {\n      dim = a.shape.length + dim;\n    }\n    // Return error if dimension is out of bounds:\n    if (dim >= a.shape.length) {\n      throw Error(\"Dimension larger than array.\");\n    }\n\n    // Build cache to use in backward step:\n    this.cache = [a, dim];\n\n    // Create output tensor:\n    const z = new Tensor(\n      _variance(a._data, dim, keepdims), // New data.\n      requiresGrad(a) // keep_dims.\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, dim] = this.cache;\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // Expand upstream gradients to the shape of \"a\":\n      dz = broadcast(dz, a);\n      // Backprop through variance:\n      const err = _add(a._data, _neg(_mean(a._data, dim, true)));\n      const var_err = _mul(_mul(dz._data, 2), err);\n      let da = _div(var_err, a.shape[dim]);\n      // Create new \"da\" Tensor:\n      da = new Tensor(da);\n      a.backward(da, z);\n    }\n  }\n}\n\n// <<< Tensor Operations >>> //\n\nexport class Transpose {\n  cache: any;\n\n  /**\n   * Transpose the tensor along two consecutive dimensions:\n   * @param {object} a - Tensor to transpose.\n   * @param {number} dim1 - First dimension.\n   * @param {number} dim2 - Second dimension.\n   * @returns {object} New tensor.\n   */\n  forward(a: Tensor, dimA: number, dimB: number): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, dimA, dimB];\n\n    // Account for negative dimension index:\n    if (dimA < 0) {\n      dimA = a.shape.length + dimA;\n    }\n    if (dimB < 0) {\n      dimB = a.shape.length + dimB;\n    }\n    // Get first dimension to be transposed:\n    let dim: number;\n    if (dimB < dimA) {\n      dim = dimB;\n    } else if (dimB > dimA) {\n      dim = dimA;\n    } else {\n      throw new Error(\"ValueError: dimensions are not consecutive.\");\n    }\n\n    // Call recursive function:\n    const z = new Tensor(\n      _transpose(a._data, dim), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, dimA, dimB] = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      const da = dz.transpose(dimA, dimB);\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class At {\n  cache: any;\n\n  forward(\n    a: Tensor,\n    idx1: Tensor | Array<any>,\n    idx2: Tensor | Array<any> | null = null\n  ): Tensor {\n    // Make sure index lists are flat JavaScript arrays:\n    if (idx1) {\n      idx1 = assureArray(idx1).flat(Infinity);\n    }\n    if (idx2) {\n      idx2 = assureArray(idx2).flat(Infinity);\n    }\n\n    // Build cache to use in backward step:\n    this.cache = [a, idx1, idx2];\n\n    // Call function:\n    const z = new Tensor(\n      _at(a._data, idx1, idx2), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, idx1, idx2] = this.cache;\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      const da = zeros(a.shape);\n      // Add derivatives to the original places from a:\n      for (let i = 0; i < dz.length; i++) {\n        // If there is a second index, add to each [i][j] coordinate (2D):\n        if (idx2 != null) {\n          da._data[idx1[i]][idx2[i]] = _add(\n            da._data[idx1[i]][idx2[i]],\n            dz._data[i]\n          );\n          // If there is not a second index, add to each [i] coordinate (1D):\n        } else {\n          da._data[idx1[i]] = _add(da._data[idx1[i]], dz._data[i]);\n        }\n      }\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class MaskedFill {\n  cache: any;\n\n  forward(\n    a: Tensor,\n    mask: Tensor,\n    condition: (someArg: number) => boolean,\n    value: number\n  ): Tensor {\n    // Build cache to use in backward step:\n    this.cache = [a, mask, condition];\n\n    // Call function:\n    const z = new Tensor(\n      _masked_fill(a._data, mask._data, condition, value), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const [a, mask, condition] = this.cache;\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // Set gradients of all reset values to zero:\n      const da = new Tensor(_masked_fill(dz._data, mask._data, condition, 0));\n\n      a.backward(da, z);\n    }\n  }\n}\n\nexport class Reshape {\n  cache: any;\n\n  forward(a: Tensor, shape: Array<number>): Tensor {\n    // Build cache to use in backward step:\n    this.cache = a;\n\n    // Call function:\n    const z = new Tensor(\n      _reshape(a._data, shape), // data;\n      requiresGrad(a) // requires_grad;\n    );\n\n    // Connect nodes in graph:\n    if (requiresGrad(a)) {\n      z.parents.push(a);\n      a.children.push(z);\n      z.operation = this;\n    }\n\n    return z;\n  }\n\n  backward(dz: Tensor, z: Tensor) {\n    // Get data from cache:\n    const a = this.cache;\n\n    // Find gradients relative to \"a\", and pass them downstream:\n    if (requiresGrad(a)) {\n      // Reshape dz back to a's shape:\n      const da = new Tensor(_reshape(dz.data, a.shape));\n      a.backward(da, z);\n    }\n  }\n}\n\n// <<< Tensor Operation Aliases >>> //\n\n/**\n * Gets the sum of the Tensor over a specified dimension.\n * @param {Tensor} a - Original Tensor.\n * @param {number} dim - Dimension to sum over.\n * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n * @returns {Tensor} - Final tensor.\n */\nexport function sum(a: Tensor, dim = -1, keepdims = false): Tensor {\n  return a.sum(dim, keepdims);\n}\n\n/**\n * Gets the mean of the Tensor over a specified dimension.\n * @param {Tensor} a - Original Tensor.\n * @param {number} dim - Dimension to get mean over.\n * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n * @returns {Tensor} - Final tensor.\n */\nexport function mean(a: Tensor, dim = -1, keepdims = false): Tensor {\n  return a.mean(dim, keepdims);\n}\n\n/**\n * Gets the variance of the Tensor over a specified dimension.\n * @param {Tensor} a - Original Tensor.\n * @param {number} dim - Dimension to get variance over.\n * @param {boolean} keepdims - Whether to keep dimensions of original tensor.\n * @returns {Tensor} - Final tensor.\n */\nexport function variance(a: Tensor, dim = -1, keepdims = false): Tensor {\n  return a.variance(dim, keepdims);\n}\n\n/**\n * Add integer or other Tensor to this Tensor.\n * @param {Tensor} a - Original Tensor.\n * @param {any} b - Tensor or integer to be added to this Tensor.\n * @returns {object} New tensor.\n */\nexport function add(a: Tensor, b: Tensor | number) {\n  return a.add(b);\n}\n\n/**\n * Subtract integer or other Tensor from this Tensor.\n * @param {Tensor} a - Original Tensor.\n * @param {any} b - Tensor or integer to be subtracted from this Tensor.\n * @returns {object} New tensor.\n */\nexport function sub(a: Tensor, b: Tensor | number) {\n  return a.sub(b);\n}\n\n/**\n * Get element-wise opposite of given tensor ( every element * (-1) )\n * @returns {object} New tensor.\n */\nexport function neg(a: Tensor) {\n  return a.neg();\n}\n\n/**\n * Multiply this Tensor by integer or other Tensor.\n * @param {any} other - Tensor or integer to multiply this Tensor by.\n * @returns {object} New tensor.\n */\nexport function mul(a: Tensor, b: Tensor | number) {\n  return a.mul(b);\n}\n\n/**\n * Divide this Tensor by integer or other Tensor.\n * @param {any} other - Tensor or integer to divide this Tensor by.\n * @returns {object} New tensor.\n */\nexport function div(a: Tensor, b: Tensor | number) {\n  const operation = new Div();\n  return operation.forward(a, b);\n}\n\n/**\n * Get tensor to element-wise power of n.\n * @param {object} a - Tensor to be elevated to the power of n.\n * @param {number} n - Exponent.\n * @returns {object} New tensor.\n */\nexport function pow(a: Tensor, n: number): Tensor {\n  const operation = new Pow();\n  return operation.forward(a, n);\n}\n\n/**\n * Get element-wise square root of given tensor.\n * @param {object} a - Tensor to be square rooted.\n * @returns {object} New tensor.\n */\nexport function sqrt(a: Tensor): Tensor {\n  return a.sqrt();\n}\n\n/**\n * Get element-wise exponentiation of given tensor ( e^(every element) )\n * @param {object} a - Tensor to be exponentiated.\n * @returns {object} New tensor.\n */\nexport function exp(a: Tensor): Tensor {\n  return a.exp();\n}\n\n/**\n * Get element-wise natural log of given tensor ( ln(every element) )\n * @param {object} a - Tensor we will take the log of.\n * @returns {object} New tensor.\n */\nexport function log(a: Tensor): Tensor {\n  return a.log();\n}\n\n/**\n * Multiply this Tensor by integer or other Tensor.\n * @param {any} other - Tensor or integer to multiply this Tensor by.\n * @returns {object} New tensor.\n */\nexport function matmul(a: Tensor, b: Tensor): Tensor {\n  return a.matmul(b);\n}\n\n/**\n * Transpose the tensor along two consecutive dimensions:\n * @param {Tensor} a - Tensor to be transposed.\n * @param {number} dim1 - First dimension.\n * @param {number} dim2 - Second dimension.\n * @returns {object} New tensor.\n */\nexport function transpose(a: Tensor, dim1: number, dim2: number): Tensor {\n  return a.transpose(dim1, dim2);\n}\n\n/**\n * In a tensor, returns a list of elements in [index1], or [index1][index2];\n * @param {Tensor} a - Original Tensor.\n * @param {object} idx1 - List containing indexes to extract data from in first dimension.\n * @param {object} idx2 - List containing indexes to extract data from in second dimension [OPTIONAL].\n * @returns {object} New tensor.\n * @example\n * let a = tensor([[1,4,2],\n *                 [6,7,8]])\n *\n * // Returns tensor([[1,4,2],\n * //                 [6,7,8],\n * //                 [1,4,2]])\n * a.at([0,1,0])\n *\n * // Returns tensor([2,6,8]):\n * a.at([0,1,1], [2,0,2])\n */\nexport function at(\n  a: Tensor,\n  idx1: Tensor | Array<any>,\n  idx2: Tensor | Array<any>\n): Tensor {\n  return a.at(idx1, idx2);\n}\n\n/**\n * Where the \"condition\" function returns True in the \"mask\" Tensor, the \"value\" will fill the \"a\" Tensor.\n * @param {Tensor} a - Original Tensor.\n * @param {Tensor} mask - \"condition\" will be applied in this tensor element-wise.\n * @param {function} condition - Function that returns True or False element-wise.\n * @param {number} value - Value to fill Tensor when condition is met.\n * @returns {object} New tensor.\n * @example\n * let a = tensor([[1,5,2,3],\n *                 [6,7,2,9]])\n *\n * // Returns tensor([[1,0,2,3],\n * //                 [0,0,2,0]])\n * masked_fill(a, mask, (el) => {return el > 3}, 0)\n */\nexport function masked_fill(\n  a: Tensor,\n  mask: Tensor,\n  condition: (someArg: number) => boolean,\n  value: number\n): Tensor {\n  return a.masked_fill(mask, condition, value);\n}\n\n/**\n * Reshape the tensor into the new shape:\n * @param {Tensor} a - Tensor to be reshaped.\n * @param {object} shape - New tensor's shape.\n * @returns {object} New tensor.\n */\nexport function reshape(a: Tensor, shape: Array<any>): Tensor {\n  return a.reshape(shape);\n}\n\n// <<< Recursive functions for lists >>> //\n\nfunction _sum(a: Array<any>, dim: number, keepdims?: boolean): Array<any> {\n  // In recursive call, when depth increases, subtract one from dim.\n  // When we reach the dimension intended (dim === 0),\n  // we add all elements in this dimension.\n  if (dim == 0) {\n    const sum = a.reduce((a, b) => _add(a, b), 0);\n    if (keepdims) {\n      return Array(a.length).fill(sum);\n    } else {\n      return sum;\n    }\n  } else if (typeof a === \"object\") {\n    return a.map((element) => _sum(element, dim - 1, keepdims));\n  } else {\n    throw Error(\"Dimension invalid.\");\n  }\n}\n\nfunction _mean(a: Array<any>, dim: number, keepdims?: boolean): Array<any> {\n  // In recursive call, when depth increases, subtract one from dim.\n  // When we reach the dimension intended (dim === 0),\n  // we add all elements in this dimension.\n  if (dim == 0) {\n    const reduced = _div(\n      a.reduce((a, b) => _add(a, b), 0),\n      a.length\n    );\n    if (keepdims) {\n      return Array(a.length).fill(reduced);\n    } else {\n      return reduced;\n    }\n  } else if (typeof a === \"object\") {\n    return a.map((element) => _mean(element, dim - 1 /*, keepdims*/));\n  } else {\n    throw Error(\"Dimension invalid.\");\n  }\n}\n\nfunction _variance(a: Array<any>, dim: number, keepdims?: boolean): Array<any> {\n  // In recursive call, when depth increases, subtract one from dim.\n  // When we reach the dimension intended (dim === 0),\n  // we add all elements in this dimension.\n  if (dim == 0) {\n    // Get mean over current dim:\n    const mean = _div(\n      a.reduce((a, b) => _add(a, b), 0),\n      a.length\n    );\n    // Get square difference to mean over current dim:\n    const squares = a.map((el) => (el - mean) ** 2);\n    // Get mean of square differences over current dim:\n    const variance = _div(\n      squares.reduce((a, b) => _add(a, b), 0),\n      a.length\n    );\n    if (keepdims) {\n      return Array(a.length).fill(variance);\n    } else {\n      return variance;\n    }\n  } else if (typeof a === \"object\") {\n    return a.map((element) => _variance(element, dim - 1 /*keepdims*/));\n  } else {\n    throw Error(\"Dimension invalid.\");\n  }\n}\n\nfunction _add(a: Array<any> | number, b: Array<any> | number): any {\n  // If both are numbers, return number. If one is a Tensor, add number to each element in tensor.\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a + b;\n  } else if (typeof a === \"number\" && b instanceof Array) {\n    return b.map((element) => _add(element, a));\n  } else if (a instanceof Array && typeof b === \"number\") {\n    return a.map((element) => _add(element, b));\n  } else if (a instanceof Array && b instanceof Array) {\n    // If both are tensors, we need to broadcast:\n    const aShape = getShape(a);\n    const bShape = getShape(b);\n    // If both have same shape, move downwards in both:\n    if (JSON.stringify(aShape) === JSON.stringify(bShape)) {\n      return a.map((element, idx) => _add(element, b[idx]));\n      // If a's shape is larger, we need to find b's shape inside a's shape:\n    } else if (aShape.length > bShape.length) {\n      let idx!: number;\n      // Look for b's shape:\n      for (let i = 0; i < aShape.length; i++) {\n        if (\n          JSON.stringify(aShape.slice(i, i + bShape.length)) ===\n          JSON.stringify(bShape)\n        ) {\n          idx = i;\n        }\n      }\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return a.map((element, idx) => _add(element, b[idx]));\n        // If not, move down only on 'a':\n      } else {\n        return a.map((element) => _add(element, b));\n      }\n      // If b's shape is larger, we need to find a's shape inside b's shape:\n    } else if (aShape.length < bShape.length) {\n      let idx!: number;\n      // Look for a's shape:\n      for (let i = 0; i < bShape.length; i++) {\n        if (\n          JSON.stringify(bShape.slice(i, i + aShape.length)) ===\n          JSON.stringify(aShape)\n        ) {\n          idx = i;\n        }\n      }\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return b.map((element, idx) => _add(a[idx], element));\n        // If not, move down only on 'b':\n      } else {\n        return b.map((element) => _add(a, element));\n      }\n    } else {\n      throw Error(\"Given arguments cannot be added.\");\n    }\n  } else {\n    throw Error(\"Given arguments cannot be added.\");\n  }\n}\n\nfunction _neg(a: Array<any> | number): Array<any> | number {\n  // If a is a number, make it negative. If not, make all of its elements negative:\n  if (typeof a === \"number\") {\n    return -a;\n  } else if (typeof a === \"object\") {\n    return a.map((element) => _neg(element));\n  } else {\n    throw new TypeError(\"the input data is not a number.\");\n  }\n}\n\nfunction _mul(a: Array<any> | number, b: Array<any> | number): any {\n  // If both are numbers, return number. If one is a Tensor, multiply each element in the tensor by the number.\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a * b;\n  } else if (typeof a === \"number\" && b instanceof Array) {\n    return b.map((element) => _mul(element, a));\n  } else if (a instanceof Array && typeof b === \"number\") {\n    return a.map((element) => _mul(element, b));\n  } else if (a instanceof Array && b instanceof Array) {\n    // If both are tensors, we need to broadcast:\n    const aShape = getShape(a);\n    const bShape = getShape(b);\n    // If both have same shape, move downwards in both:\n    if (JSON.stringify(aShape) === JSON.stringify(bShape)) {\n      return a.map((element, idx) => _mul(element, b[idx]));\n      // If a's shape is larger, we need to find b's shape inside a's shape:\n    } else if (aShape.length > bShape.length) {\n      let idx;\n      // Look for b's shape:\n      for (let i = 0; i < aShape.length; i++) {\n        if (\n          JSON.stringify(aShape.slice(i, i + bShape.length)) ===\n          JSON.stringify(bShape)\n        ) {\n          idx = i;\n        }\n      }\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return a.map((element, idx) => _mul(element, b[idx]));\n        // If not, move down only on 'a':\n      } else {\n        return a.map((element) => _mul(element, b));\n      }\n      // If b's shape is larger, we need to find a's shape inside b's shape:\n    } else if (aShape.length < bShape.length) {\n      let idx;\n      // Look for a's shape:\n      for (let i = 0; i < bShape.length; i++) {\n        if (\n          JSON.stringify(bShape.slice(i, i + aShape.length)) ===\n          JSON.stringify(aShape)\n        ) {\n          idx = i;\n        }\n      }\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return b.map((element, idx) => _mul(a[idx], element));\n        // If not, move down only on 'b':\n      } else {\n        return b.map((element) => _mul(a, element));\n      }\n    }\n  }\n}\n\nfunction _div(a: Array<any> | number, b: Array<any> | number): any {\n  // If both are numbers, return number. If one is a Tensor, divide each element in the tensor by the number.\n  if (typeof a === \"number\" && typeof b === \"number\") {\n    return a / b;\n  } else if (typeof a === \"number\" && b instanceof Array) {\n    return b.map((element) => _div(a, element));\n  } else if (a instanceof Array && typeof b === \"number\") {\n    return a.map((element) => _div(element, b));\n  } else if (a instanceof Array && b instanceof Array) {\n    // If both are tensors, we need to broadcast:\n    const aShape = getShape(a);\n    const bShape = getShape(b);\n    // If both have same shape, move downwards in both:\n    if (JSON.stringify(aShape) === JSON.stringify(bShape)) {\n      return a.map((element, idx) => _div(element, b[idx]));\n      // If a's shape is larger, we need to find b's shape inside a's shape:\n    } else if (aShape.length > bShape.length) {\n      let idx;\n      // Look for b's shape:\n      for (let i = 0; i < aShape.length; i++) {\n        if (\n          JSON.stringify(aShape.slice(i, i + bShape.length)) ===\n          JSON.stringify(bShape)\n        ) {\n          idx = i;\n        }\n      }\n\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return a.map((element, idx) => _div(element, b[idx]));\n        // If not, move down only on 'a':\n      } else {\n        return a.map((element) => _div(element, b));\n      }\n      // If b's shape is larger, we need to find a's shape inside b's shape:\n    } else if (aShape.length < bShape.length) {\n      let idx;\n      // Look for a's shape:\n      for (let i = 0; i < bShape.length; i++) {\n        if (\n          JSON.stringify(bShape.slice(i, i + aShape.length)) ===\n          JSON.stringify(aShape)\n        ) {\n          idx = i;\n        }\n      }\n      // If it's right on top of the array, move down on both:\n      if (idx === 0) {\n        return b.map((element, idx) => _div(a[idx], element));\n        // If not, move down only on 'b':\n      } else {\n        return b.map((element) => _div(a, element));\n      }\n    }\n  }\n}\n\nfunction _matmul(a: Array<any>, b: Array<any>, kernel: any): Array<any> {\n  if (typeof a === \"number\") {\n    throw new Error(\"Cannot perform MatMul with given shapes.\");\n  }\n  // If this dimension has equal lengths, keep searching:\n  if (typeof a[0][0] === \"object\") {\n    return a.map((element: Array<any>, idx: number) =>\n      _matmul(element, b[idx], kernel)\n    );\n    // If not, try to matmul:\n  } else {\n    // If dimensions align, perform matmul:\n    if (a[0].length === b.length && typeof a[0][0] === \"number\") {\n      let out = kernel(a, b, b.length);\n      out = out.map((el: number[]) => Array.from(el));\n      return out;\n    } else {\n      throw Error(\n        `Cannot perform Matrix Multiplication: cannot broadcast ${[\n          a.length,\n          a[0].length\n        ]} and ${[b.length, b[0].length]}`\n      );\n    }\n  }\n}\n// =================== NUEVO ========================= //\n\nfunction _pow(a: Array<any> | number, n: number): Array<any> | number {\n  // If a is a number, exponentiate it. If not, exponentiate all of its elements:\n  let z = a;\n  for (let i = 0; i < n - 1; i++) {\n    z = _mul(z, a);\n  }\n  return z;\n}\n\nfunction _sqrt(a: Array<any> | number): Array<any> | number {\n  // If a is a number, take square root of it. If not, take root of all of its elements:\n  if (typeof a === \"number\") {\n    return Math.sqrt(a);\n  } else if (a instanceof Array) {\n    return a.map((element: Array<any>) => _sqrt(element));\n  } else {\n    throw new TypeError(\"the input data is not a number.\");\n  }\n}\n\nfunction _exp(a: Array<any> | number): Array<any> | number {\n  // If a is a number, exponentiate it. If not, exponentiate all of its elements:\n  if (typeof a === \"number\") {\n    return 2.718281828459045 ** a;\n  } else if (a instanceof Array) {\n    return a.map((element: Array<any>) => _exp(element));\n  } else {\n    throw new TypeError(\"the input data is not a number.\");\n  }\n}\n\nfunction _log(a: Array<any> | number): Array<any> | number {\n  // If a is a number, take it's log. If not, take log of all of it's elements:\n  if (typeof a === \"number\") {\n    return Math.log(a);\n  } else if (a instanceof Array) {\n    return a.map((element: Array<any>) => _log(element));\n  } else {\n    throw new TypeError(\"the input data is not a number.\");\n  }\n}\n\nfunction _transpose(a: Array<any>, dim: number): Array<any> {\n  // Go down the dimensions recursively until we get to the dimension to be transposed:\n  if (dim == 0) {\n    // Build array with the transposed shape (to be filled with transposed values):\n    const newArray = Array(a[0].length)\n      .fill(0)\n      .map(() => Array(a.length).fill(0));\n\n    for (let i = 0; i < a.length; i++) {\n      for (let j = 0; j < a[i].length; j++) {\n        newArray[j][i] = a[i][j];\n      }\n    }\n    return newArray;\n  } else if (a instanceof Array) {\n    return a.map((element: Array<any>) => _transpose(element, dim - 1));\n  } else {\n    throw Error(\"ValueError: dimensions are invalid.\");\n  }\n}\n\nfunction _at(\n  a: Array<any>,\n  idx1: Array<any>,\n  idx2: Array<any> | null\n): Array<any> {\n  // If there is a second index, fill a new array in position \"N\" with a[idx1[N]][idx2[N]] (2 Dims):\n  if (idx2) {\n    return Array(idx1.length)\n      .fill(0)\n      .map((_, i) => a[idx1[i]][idx2[i]]);\n    // If there is no second index, fill a new array in position \"N\" with a[idx1[N]] (1 Dim):\n  } else {\n    return Array(idx1.length)\n      .fill(0)\n      .map((_, i) => a[idx1[i]]);\n  }\n}\n\nfunction _masked_fill(\n  a: Array<any> | number,\n  mask: Array<any> | number,\n  condition: (someArg: number) => boolean,\n  value: number\n): Array<any> | number {\n  // If a is a number, test \"condition\" on it. If not, recursive step to all of its elements:\n  if (typeof mask === \"number\") {\n    if (typeof a != \"number\") {\n      throw new Error(\"Tensor and Mask not broadcastable\");\n    }\n    if (condition(mask)) {\n      return value;\n    } else {\n      return a;\n    }\n  } else if (typeof a === \"object\") {\n    return a.map((element, idx) =>\n      _masked_fill(element, mask[idx], condition, value)\n    );\n  } else {\n    throw new Error(\"The input data is not a number.\");\n  }\n}\n\n// export function _reshape(a: Array<any>, shape: Array<number>): Array<any> {\n//   // Rebuilds flattened array \"flat\" with shape \"shape\":\n//   function _build(a: any[], shape: any[]): Array<any> {\n//     if (shape.length > 1) {\n//       const emptyArray = Array(shape[0]).fill(0);\n//       return emptyArray.map(() => _build(a, shape.slice(1)));\n//     } else {\n//       const emptyArray = Array(shape[0]).fill(0);\n//       return emptyArray.map(() => a.shift());\n//     }\n//   }\n\n//   // Flatten array with a's data:\n//   const flat = a.flat(Infinity);\n//   // Rebuild a with new shape:\n//   return _build(flat, shape);\n// }\n\nexport function _reshape(a: Array<any>, shape: number[]) {\n  if (\n    getShape(a).reduce((a, b) => a * b, 1) != shape.reduce((a, b) => a * b, 1)\n  ) {\n    throw new Error(\"Attempting to reshape into invalid shape.\");\n  }\n  function _build(\n    a2: any[],\n    shape2: number[],\n    idx: number,\n    numberOfEls: number\n  ): any[] {\n    if (shape2.length > 1) {\n      const emptyArray = Array(shape2[0]).fill(0);\n      let offSet = idx;\n      numberOfEls = numberOfEls / shape2[0];\n      const myArray = emptyArray.map((_, idx) =>\n        _build(a2, shape2.slice(1), offSet + idx * numberOfEls, numberOfEls)\n      );\n      return myArray;\n    } else {\n      const myArray = a2.slice(idx, idx + numberOfEls);\n      return myArray;\n    }\n  }\n  const flat = a.flat(Infinity);\n  const built = _build(flat, shape, 0, flat.length);\n  return built;\n}\n\n// <<< Tensor Initialization Methods >>> //\n\n/**\n * Generic initializer, creates new instance of the Tensor class, filling up a shape with a value.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {function} valueFunc - Function that returns number to fill up the Tensor.\n * @returns {object} New tensor.\n */\nfunction _tensorInitializer(\n  shape: Array<number>,\n  valueFunc: () => number\n): Array<any> {\n  if (shape.length === 1) {\n    const emptyArray = Array(shape[0]).fill(0);\n    return emptyArray.map(() => valueFunc());\n  } else {\n    const currentSize = shape[0];\n    const emptyArray = Array(currentSize).fill(0);\n    return emptyArray.map(() => _tensorInitializer(shape.slice(1), valueFunc));\n  }\n}\n\n/**\n * Creates new instance of the Tensor class.\n * @param {object} data - Iterable containing the data to be stored in the Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @returns {object} New tensor.\n */\nexport function tensor(\n  data: Array<any>,\n  requires_grad = false,\n  device = \"cpu\"\n): Tensor {\n  return new Tensor(data, requires_grad, device);\n}\n\n/**\n * Creates new instance of the Tensor class filled with only zeros.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @returns {object} New tensor.\n */\nexport function zeros(\n  shape: Array<number>,\n  requires_grad = false,\n  device = \"cpu\"\n): Tensor {\n  return new Tensor(\n    _tensorInitializer(shape, () => 0),\n    requires_grad,\n    device\n  );\n}\n\n/**\n * Creates new instance of the Tensor class filled with only ones.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @returns {object} New tensor.\n */\nexport function ones(\n  shape: Array<number>,\n  requires_grad = false,\n  device = \"cpu\"\n): Tensor {\n  return new Tensor(\n    _tensorInitializer(shape, () => 1),\n    requires_grad,\n    device\n  );\n}\n\n/**\n * Creates new instance of a lower-triangular 2D Tensor.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @returns {object} New tensor.\n */\nexport function tril(\n  shape: Array<number>,\n  requires_grad = false,\n  device = \"cpu\"\n): Tensor {\n  const z = ones(shape, requires_grad);\n  for (let i = 0; i < shape[0]; i++) {\n    for (let j = 0; j < shape[0]; j++) {\n      if (j > i) {\n        z._data[i][j] = 0;\n      }\n    }\n  }\n\n  return new Tensor(z._data, requires_grad, device);\n}\n\n/**\n * Creates new instance of the Tensor class filled with numbers in a uniform distribution in ]0,1[.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @returns {object} New tensor.\n */\nexport function rand(\n  shape: Array<number>,\n  requires_grad = false,\n  device = \"cpu\"\n): Tensor {\n  return new Tensor(\n    _tensorInitializer(shape, () => Math.random()),\n    requires_grad,\n    device\n  );\n}\n\n/**\n * Creates new instance of the Tensor class filled with numbers in a normal distribution.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @param {string} device - Device to store Tensor. Either \"gpu\" or \"cpu\".\n * @param {boolean} xavier - Whether to use xavier initialization (divide by square root of first input dimension).\n * @returns {object} New tensor.\n */\nexport function randn(\n  shape: Array<number>,\n  requires_grad = false,\n  device = \"cpu\",\n  xavier = false\n): Tensor {\n  return new Tensor(\n    _tensorInitializer(shape, () => {\n      const mean = Math.random() * 0.98 + 1e-3;\n      const variance = Math.random() * 0.98 + 1e-3;\n      const num =\n        Math.sqrt(-2.0 * Math.log(mean)) * Math.cos(2.0 * Math.PI * variance);\n      if (xavier) {\n        // Apply Xavier initialization to control scalar sizes:\n        return num / Math.sqrt(shape[0]);\n      } else {\n        return num;\n      }\n    }),\n    requires_grad,\n    device\n  );\n}\n\n/**\n * Creates new instance of the Tensor class filled with random integers between low and high.\n * @param {number} low - Lowest number that can be sampled.\n * @param {number} high - One above highest number that can be sampled.\n * @param {object} shape - List containing the shape of the new tensor Tensor.\n * @param {boolean} requires_grad - Whether to keep track of this tensor's gradients.\n * @returns {object} New tensor.\n */\nexport function randint(\n  low = 0,\n  high = 1,\n  shape = [1],\n  requires_grad = false\n): Tensor {\n  return new Tensor(\n    _tensorInitializer(shape, () => {\n      return Math.floor(Math.random() * (high - low)) + low;\n    }),\n    requires_grad\n  );\n}\n\n// <<< Helper Functions >>> //\n\n/**\n * Returns if a variable requires gradient tracking.\n * @param {any} - Variable to check if requires_grad.\n * @returns {boolean} Whether to track gradients.\n */\nexport function requiresGrad(a: Tensor | number | Array<any>): boolean {\n  if (a instanceof Tensor) {\n    return a.requires_grad;\n  } else {\n    return false;\n  }\n}\n\n/**\n * Broadcasts tensor \"a\" into shape of \"b\".\n * If the shape gets smaller, tensor will be summed. If it gets larger, tensor will be expanded.\n * @param {object} a - First tensor, will be broadcast into shape of second.\n * @param {object} b - Second tensor.\n * @returns {object} New tensor.\n * @example\n * // Returns tensor with shape [4,3,2]:\n * broadcast(randn([3,2]), randn([4,3,2]));\n *\n * // Returns tensor with shape [4,5,3,1]:\n * broadcast(ones([5,3,2]), ones([4,5,3,1]));\n */\nexport function broadcast(a: Tensor, b: Tensor): Tensor {\n  function _broadcast(\n    out: Array<any> | number,\n    b: Array<any> | number\n  ): Array<any> | number {\n    if (typeof out === \"number\" && typeof b === \"number\") {\n      return out;\n    } else if (typeof out === \"number\" && b instanceof Array) {\n      const newArray = Array(b.length).fill(out);\n      return _broadcast(newArray, b);\n    } else if (out instanceof Array && typeof b === \"number\") {\n      return _broadcast(_sum(out, 0), b);\n    } else if (JSON.stringify(getShape(out)) === JSON.stringify(getShape(b))) {\n      return out;\n    } else if (out instanceof Array && b instanceof Array) {\n      // If both are tensors, we need to broadcast:\n      const outShape = getShape(out);\n      const bShape = getShape(b);\n      // If out's shape is larger, we need to find b's shape inside out's shape:\n      if (outShape.length > bShape.length) {\n        let idx!: number;\n        // Look for b's shape:\n        for (let i = 0; i < outShape.length; i++) {\n          if (\n            JSON.stringify(outShape.slice(i, i + bShape.length)) ===\n            JSON.stringify(bShape)\n          ) {\n            idx = i;\n          }\n        }\n        // If it's right on top of the array, move down on both:\n        if (idx === 0) {\n          return out.map((element, idx) => _broadcast(element, b[idx]));\n          // If not, move down only on 'out':\n        } else {\n          //return out.map((element) => _broadcast(element, b));\n          return _sum(out, 0);\n        }\n        // If b's shape is larger, we need to find out's shape inside b's shape:\n      } else if (outShape.length < bShape.length) {\n        let idx!: number;\n        // Look for out's shape:\n        for (let i = 0; i < bShape.length; i++) {\n          if (\n            JSON.stringify(bShape.slice(i, i + outShape.length)) ===\n            JSON.stringify(outShape)\n          ) {\n            idx = i;\n          }\n        }\n        // If it's right on top of the array, move down on both:\n        if (idx === 0) {\n          return out.map((element) => _broadcast(element, b[0]));\n          // If not, move down only on 'b':\n        } else {\n          return Array(b.length)\n            .fill(0)\n            .map(() => _broadcast(out, b[0]));\n        }\n      } else {\n        // Define recursive function to find dimension with length 1:\n        const _broadcastSideways = (\n          out: Array<any> | number | null,\n          b: Array<any>\n        ): Array<any> => {\n          if (out instanceof Array && b.length != out.length) {\n            if (b.length === 1) {\n              // Base case, contract existing dimension:\n              return [_sum(out, 0)];\n            } else if (out.length === 1) {\n              // Base case, expand existing dimension:\n              const emptyArray = Array(b.length).fill(zeros);\n              return emptyArray.map(() => out[0]);\n            } else {\n              throw Error(\n                `Shapes ${getShape(out)} and ${getShape(b)} not broadcastable.`\n              );\n            }\n          } else {\n            // Recursive case:\n            if (out instanceof Array) {\n              // Keep looking inside each element:\n              return out.map((element: Array<any>, idx: number) =>\n                _broadcastSideways(element, b[idx])\n              );\n            } else if (typeof out === \"number\") {\n              // In case the element is a number:\n              return [null].map((element, idx) =>\n                _broadcastSideways(element, b[idx])\n              );\n            } else {\n              throw Error(\"Shapes not broadcastable.\");\n            }\n          }\n        };\n        // Return final broadcast tensor:\n        return _broadcastSideways(out, b);\n      }\n    } else {\n      throw Error(\"Shapes not broadcastable.\");\n    }\n  }\n\n  let out = a.data;\n  while (JSON.stringify(getShape(out)) != JSON.stringify(b.shape)) {\n    out = assureArray(_broadcast(out, b.data));\n  }\n  return new Tensor(out);\n}\n\n/**\n * Adds new dimensions to \"a\" until it's depth matches \"b\".\n * @param {object} inElement - First tensor, will be broadcast into dims of second.\n * @param {object} outElement - Second tensor.\n * @returns {object} New tensor.\n * @example\n * // Returns tensor with shape [4,2,3]:\n * broadcastUp(ones([2,3]), ones([4,3,2]));\n */\nexport function broadcastUp(\n  inElement: Array<any>,\n  outElement: Array<any>\n): Array<any> {\n  function _broadcastUp(\n    inElement: Array<any>,\n    outElement: Array<any>\n  ): Array<any> {\n    if (getShape(inElement).length + 1 === getShape(outElement).length) {\n      // Base case, create new dimension:\n      const emptyArray = Array(outElement.length).fill(zeros);\n      return emptyArray.map(() => inElement);\n    } else {\n      // Recursive case. Keep looking inside each element:\n      const emptyArray = Array(outElement.length).fill(zeros);\n      return emptyArray.map((_, idx) =>\n        _broadcastUp(inElement, outElement[idx])\n      );\n    }\n  }\n  while (getShape(inElement).length < getShape(outElement).length) {\n    inElement = _broadcastUp(inElement, outElement);\n  }\n  return inElement;\n}\n","import {\n  Parameter,\n  Tensor,\n  randn,\n  zeros,\n  tril,\n  broadcast,\n  tensor,\n  exp,\n  rand,\n  ones,\n  sqrt,\n  mul,\n  log,\n  _reshape\n} from \"./tensor\";\nimport fs from \"fs\";\n\n// Interface that contains all the types of Module's attributes:\ninterface ModuleInterface {\n  // Array of [key: values] of the properties of the Module:\n  [key: string]: Module | Parameter | Tensor | any;\n  parameters(): (Parameter | Tensor)[];\n  train(): void;\n  eval(): void;\n  entries(): [string, Module | Parameter | Tensor | any][];\n  mode: \"train\" | \"eval\";\n}\n\n// Module class:\nexport class Module implements ModuleInterface {\n  // Instantiate Module's learnable parameters:\n  [key: string]: Module | Parameter | Tensor | any;\n  // Instantiate Module's mode initially as \"train\":\n  mode: \"train\" | \"eval\" = \"train\";\n\n  /**\n   * Returns all model parameters in a list.\n   * @returns {object} List with parameters in the model.\n   */\n  parameters(): (Parameter | Tensor)[] {\n    // Iterate over each item in this Module.\n    let params: (Parameter | Tensor)[] = [];\n    for (const [_, value] of this.entries()) {\n      // Add every Module, Parameter or Tensor with requires_grad set to True:\n      if (value instanceof Module) {\n        params = params.concat(value.parameters());\n      } else if (value instanceof Parameter) {\n        params.push(value);\n      } else if (value instanceof Tensor) {\n        if (value.requires_grad) {\n          params.push(value);\n        }\n      }\n    }\n    return params;\n  }\n\n  /**\n   * Sets module's mode to train, which influences layers like Dropout\n   */\n  train() {\n    this.mode = \"train\";\n    for (const [_, param] of this.entries()) {\n      if (param instanceof Module) {\n        param.train();\n      }\n    }\n  }\n\n  /**\n   * Sets module's mode to eval, which influences layers like Dropout\n   */\n  eval() {\n    this.mode = \"eval\";\n    for (const [_, param] of this.entries()) {\n      if (param instanceof Module) {\n        param.eval();\n      }\n    }\n  }\n\n  /**\n   * Returns an array of key/values of the enumerable properties of the Module\n   * @returns {object} List with parameters in the model.\n   */\n  entries(): [string, Module | Parameter | Tensor | any][] {\n    return Object.entries(this);\n  }\n}\n\n// Standard Layers:\n\nexport class Linear extends Module {\n  public W: Tensor;\n  public b: Tensor;\n  public has_bias: boolean;\n  /**\n   * Simple linear layer, with weight matrix and optional bias. Does not contain nonlinearity.\n   *\n   * @param {number} in_size - size of the last dimention of the input array.\n   * @param {number} out_size - size of the last dimention of the output array.\n   * @param {string} device - Device to perform Tensor operations. Either \"gpu\" or \"cpu\".\n   * @param {boolean} bias - wether to include a bias term.\n   * @param {boolean} xavier - Wether to use xavier initialization (divide by square root of first input dimension).\n   */\n  constructor(\n    in_size: number,\n    out_size: number,\n    device = \"cpu\",\n    bias = true,\n    xavier = true\n  ) {\n    super();\n    this.W = randn([in_size, out_size], true, device, xavier);\n    this.b = zeros([out_size], true);\n    this.has_bias = bias;\n  }\n\n  /**\n   * Performs forward pass through the Linear layer.\n   * @param {Tensor} x - input Tensor.\n   * @returns {Tensor} new Tensor. Out = (In @ W) + b.\n   */\n  forward(x: Tensor): Tensor {\n    let z = x.matmul(this.W);\n    if (this.has_bias) {\n      z = z.add(this.b);\n    }\n    console.log(\"heyy taylor\");\n    return z;\n  }\n}\n\nexport class MultiHeadSelfAttention extends Module {\n  public Wk: Linear;\n  public Wq: Linear;\n  public Wv: Linear;\n  public residual_proj: Linear;\n  public mask: Tensor;\n  public att_dropout: Dropout;\n  public residual_dropout: Dropout;\n  public softmax: Softmax;\n  public H: number;\n\n  /**\n   * Full transformer Layer implementation.\n   *\n   * @param {number} in_size - size of the last dimention of the input array.\n   * @param {number} out_size - size of the last dimention of the output array.\n   * @param {number} n_heads - number of parallel heads to be computed (must equally divide in_size).\n   * @param {number} n_timesteps - length of text sequence to be processed bt Transformer.\n   * @param {number} dropout_prob - probability of zeroing each activation in dropout Layer.\n   * @param {string} device - Device to perform Tensor operations. Either \"gpu\" or \"cpu\".\n   */\n  constructor(\n    in_size: number,\n    out_size: number,\n    n_heads: number,\n    n_timesteps: number,\n    dropout_prob = 0,\n    device = \"cpu\"\n  ) {\n    super();\n    this.Wk = new Linear(in_size, in_size, device, true, false);\n    this.Wq = new Linear(in_size, in_size, device, true, false);\n    this.Wv = new Linear(in_size, in_size, device, true, false);\n    this.residual_proj = new Linear(in_size, out_size, device, true, false);\n    this.mask = tril([n_timesteps, n_timesteps], false);\n    this.att_dropout = new Dropout(dropout_prob);\n    this.residual_dropout = new Dropout(dropout_prob);\n    this.softmax = new Softmax();\n\n    // Store head_size and verify that it's an integer:\n    this.H = in_size / n_heads;\n    if (in_size % n_heads != 0) {\n      throw new Error(\"Embedding dimension not divisible in equal heads.\");\n    }\n  }\n\n  /**\n   * Performs Multi Head Self-Attention on \"x\" tensor.\n   * @param {Tensor} x - input Tensor.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(x: Tensor): Tensor {\n    const [B, T, D] = x.shape;\n    const H = this.H;\n    const nh = D / H; // Num heads\n\n    // Get key, queries and values from the input:\n    let k = this.Wk.forward(x); // (B, T, D) @ (D, D) -> (B, T, D)\n    let q = this.Wq.forward(x); // (B, T, D) @ (D, D) -> (B, T, D)\n    let v = this.Wv.forward(x); // (B, T, D) @ (D, D) -> (B, T, D)\n\n    // Reshape into different heads:\n    k = k.reshape([B, T, nh, H]).transpose(1, 2); // (B, T, D) -> (B, T, nh, H) -> (B, nh, T, H)\n    q = q.reshape([B, T, nh, H]).transpose(1, 2); // (B, T, D) -> (B, T, nh, H) -> (B, nh, T, H)\n    v = v.reshape([B, T, nh, H]).transpose(1, 2); // (B, T, D) -> (B, T, nh, H) -> (B, nh, T, H)\n\n    // Compute attention activation:\n    const kT = k.transpose(-2, -1);\n    let att = q.matmul(kT); // (B, nh, T, H) @ (B, nh, H, T) -> (B, nh, T, T)\n\n    // Reduce module before going into softmax:\n    att = att.div(H ** 2);\n\n    // Apply mask (to block out future characters), softmax, and dropout:\n    const mask = broadcast(this.mask, att);\n    att = att.masked_fill(mask, (el: number): boolean => el === 0, -Infinity);\n    att = this.softmax.forward(att, -1);\n    att = this.att_dropout.forward(att);\n\n    // Compute weighted sum between values:\n    let out = att.matmul(v); // (B, nh, T, T) @ (B, nh, T, H) -> (B, nh, T, H)\n\n    // Restack heads in D dimension:\n    out = out.transpose(1, 2).reshape([B, T, D]); // (B, nh, T, H) -> (B, T, D)\n\n    // Apply final projection (Dense layer) and dropout:\n    out = this.residual_proj.forward(out); // (B, T, D) @ (D, D) -> (B, T, D)\n    out = this.residual_dropout.forward(out);\n\n    return out;\n  }\n}\n\nexport class FullyConnected extends Module {\n  public l1: Linear;\n  public relu: ReLU;\n  public l2: Linear;\n  public dropout: Dropout;\n  /**\n   * Small block composed of two Linear layers, a ReLU non-linearity and a Dropout layer.\n   *\n   * @param {number} in_size - size of the last dimention of the input array.\n   * @param {number} out_size - size of the last dimention of the output array.\n   * @param {number} dropout_prob - probability of zeroing each activation in dropout Layer.\n   * @param {string} device - Device to perform Tensor operations. Either \"gpu\" or \"cpu\".\n   * @param {boolean} bias - wether to include a bias term.\n   */\n  constructor(\n    in_size: number,\n    out_size: number,\n    dropout_prob = 0,\n    device: string = \"cpu\",\n    bias: boolean = true\n  ) {\n    super();\n\n    this.l1 = new Linear(in_size, in_size * 2, device, true, bias);\n    this.relu = new ReLU();\n    this.l2 = new Linear(in_size * 2, out_size);\n    this.dropout = new Dropout(dropout_prob);\n  }\n\n  /**\n   *  Passes \"x\" tensor through the Fully Connected layers.\n   * @param {Tensor} x - input Tensor.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(x: Tensor): Tensor {\n    let z = this.l1.forward(x);\n    z = this.relu.forward(z);\n    z = this.l2.forward(z);\n    z = this.dropout.forward(z);\n    return z;\n  }\n}\n\nexport class Block extends Module {\n  public att: MultiHeadSelfAttention;\n  public ln1: LayerNorm;\n  public fcc: FullyConnected;\n  public ln2: LayerNorm;\n\n  /**\n   * Full transformer decoder block. Composed of Multi Head Self Attention, Fully connected layers and Layer Norms.\n   *\n   * @param {number} in_size - size of the last dimention of the input array.\n   * @param {number} out_size - size of the last dimention of the output array.\n   * @param {number} n_heads - number of parallel heads to be computed (must equally divide in_size).\n   * @param {number} n_timesteps - length of text sequence to be processed bt Transformer.\n   * @param {number} dropout_prob - probability of zeroing each activation in dropout Layer.\n   * @param {string} device - Device to perform Tensor operations. Either \"gpu\" or \"cpu\".\n   */\n  constructor(\n    in_size: number,\n    out_size: number,\n    n_heads: number,\n    n_timesteps: number,\n    dropout_prob = 0,\n    device = \"cpu\"\n  ) {\n    super();\n    this.att = new MultiHeadSelfAttention(\n      in_size,\n      in_size,\n      n_heads,\n      n_timesteps,\n      dropout_prob,\n      device\n    );\n    this.ln1 = new LayerNorm(in_size);\n    this.fcc = new FullyConnected(\n      in_size,\n      out_size,\n      dropout_prob,\n      device,\n      true\n    );\n    this.ln2 = new LayerNorm(out_size);\n  }\n\n  /**\n   * Passes \"x\" tensor through a full transformer Block.\n   * @param {Tensor} x - input Tensor.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(x: Tensor): Tensor {\n    let z = x.add(this.att.forward(this.ln1.forward(x)));\n    //z = this.ln1.forward(z)\n    z = z.add(this.fcc.forward(this.ln2.forward(z)));\n    //z = this.ln2.forward(z);\n    return z;\n  }\n}\n\n// Embedding Layers\n\nexport class Embedding extends Module {\n  public E: Tensor;\n\n  /**\n   * Embedding class, turns indexes into vectors.\n   *\n   * @param {number} vocab_size - number of different indexes (vocabulary size).\n   * @param {number} embed_size - size of the embedding vector generated.\n   */\n  constructor(vocab_size: number, embed_size: number) {\n    super();\n    this.E = randn([vocab_size, embed_size], true, \"cpu\", false);\n  }\n\n  /**\n   * Extracts embedding from rows in \"idx\":\n   * @param {Tensor} idx - rows to get embedding from.\n   * @returns {Tensor} new Tensor. Out = (In @ W) + b.\n   */\n  forward(idx: Tensor): Tensor {\n    // Get idx dimensions:\n    const [B, T] = idx.shape;\n\n    let x = this.E.at(idx);\n\n    // Assure output tensor has desired shape:\n    x = x.reshape([B, T, this.E.shape[1]]);\n\n    return x;\n  }\n}\n\nexport class PositionalEmbedding extends Module {\n  public E: Tensor;\n\n  /**\n   * Embedding class, turns indexes into vectors based on it's position through an optimized lookup table.\n   *\n   * @param {number} input_size - number of different embeddings (size of the input).\n   * @param {number} embed_size - size of the embedding vector generated.\n   */\n  constructor(input_size: number, embed_size: number) {\n    super();\n    this.E = randn([input_size, embed_size], true, \"cpu\", false);\n  }\n\n  /**\n   * Gets embedding for timesteps in \"idx\" array.\n   * @param {object} idx - Array [Batch x Timesteps]. Timesteps will be filled with positional embeddings.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(idx: Tensor): Tensor {\n    // Get num_timesteps dimension:\n    const [_, T] = idx.shape;\n    // Creates positional embeddings: (Batch, Timesteps) => (Batch, Timesteps, Embed)\n    const x = this.E.at([...Array(T).keys()]);\n\n    return x;\n  }\n}\n\n// Non-linearity Layers:\n\nexport class ReLU extends Module {\n  /**\n   * Rectified Linear Unit nonlinearity. Returns z if z>0 else 0.\n   */\n  constructor() {\n    super();\n  }\n\n  /**\n   * Performs forward pass through Rectified Linear Unit nonlinearity. Returns z if z>0 else 0.\n   * @param {Tensor} z - input Tensor.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(z: Tensor): Tensor {\n    // Define recursive function:\n    function _relu(z: Array<any>): Array<any> {\n      // Base case, perform ReLU:\n      if (typeof z[0] === \"number\") {\n        return z.map((el: number): number => {\n          if (el > 0) {\n            return 1.0;\n          } else {\n            return 0.001;\n          }\n        });\n        // Recursive case, go deeper in array:\n      } else if (typeof z[0] === \"object\") {\n        return z.map((el: Array<any>): Array<any> => _relu(el));\n      } else throw Error(\"In ReLU, provided Tensor is not homogenous.\");\n    }\n    const mask = tensor(_relu(z._data));\n\n    z = z.mul(mask);\n    return z;\n  }\n}\n\nexport class Softmax extends Module {\n  /**\n   * Softmax nonlinearity class. Returns distribution of values (sum=1).\n   */\n  constructor() {\n    super();\n  }\n\n  /**\n   * Performs forward pass through Softmax nonlinearity.\n   * @param {Tensor} z - input Tensor.\n   * @param {number} dim - dimension across which to apply Softmax.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(z: Tensor, dim = -1): Tensor {\n    z = exp(z);\n    const out = z.div(z.sum(dim, true));\n    return out;\n  }\n}\n\n// Regularization Layers:\n\nexport class Dropout extends Module {\n  public p: number;\n\n  /**\n   * Dropout class, added usually after other layers, to drop values to zero with given probability\n   *\n   * @param {number} drop_prob - probability to drop each value in input.\n   */\n  constructor(drop_prob: number) {\n    super();\n    this.p = drop_prob;\n    this.mode = \"train\";\n  }\n  /**\n   * Performs forward pass through Dropout layer. Sets random values to zero (this.p % of the total).\n   * @param {Tensor} z - input Tensor.\n   * @returns {Tensor} new Tensor.\n   */\n  forward(z: Tensor): Tensor {\n    if (this.mode == \"eval\") {\n      return z;\n    }\n    const mask = rand(z.shape);\n    // Set to zero all values of uniform distribution lower than probability of dropout:\n    let a = z.masked_fill(\n      mask,\n      (el: number): boolean => {\n        return el < this.p;\n      },\n      0\n    );\n    // Scale modulus by probability during training time:\n    a = a.div(1 - this.p);\n    return a;\n  }\n}\n\nexport class LayerNorm extends Module {\n  public gamma: Tensor;\n  public beta: Tensor;\n\n  /**\n   * Layer Norm class, added usually after other layers to normalize across all of the output.\n   *\n   * @param {number} n_embed - size of the last dimention of the input.\n   */\n  constructor(n_embed: number) {\n    super();\n    this.gamma = ones([n_embed], true);\n    this.beta = zeros([n_embed], true);\n  }\n\n  forward(x: Tensor): Tensor {\n    const var_x = x.variance(-1, true); // (B, T)\n    const norm_x = x.sub(x.mean(-1, true)).div(sqrt(var_x)); // (B, T, D)\n    const z = mul(norm_x, this.gamma).add(this.beta); // (B, T, D)\n    return z;\n  }\n}\n\n// Loss layers:\n\nexport class CrossEntropyLoss extends Module {\n  /**\n   * Cross Entropy Loss class, returns the loss given the output and the expected indexes.\n   */\n  constructor() {\n    super();\n  }\n\n  /**\n   * Performs forward pass through CrossEntropyLoss, returns loss.\n   * @param {Tensor} z - Output from the last layer of the network. Must have shape like (*Batch dimentions, Number of possible classes).\n   * @param {object} y - Correct indexes expected from the model.\n   * @returns {object} Negative-log-likelihood loss of the model output.\n   */\n  forward(z: Tensor, y: Tensor): Tensor {\n    // Get data's shape:\n    let zDims = z.shape;\n    // Get last dimension:\n    const D = zDims.slice(zDims.length - 1, zDims.length)[0];\n    // Get product of all batch dimensions:\n    zDims = zDims.slice(0, zDims.length - 1);\n    const B = zDims.reduce((a, b) => a * b, 1);\n    // Flatten out the batch dimensions:\n    z = z.reshape([B, D]);\n\n    // Perform softmax on output:\n    const logitsExp = exp(z);\n\n    const logitsSum = logitsExp.sum(1, true);\n\n    const logits = logitsExp.div(logitsSum);\n\n    const y_array = _reshape(y.data, [B]);\n\n    // Get cross-entropy loss:\n    const at_logits = logits.at([...Array(B).keys()], y_array);\n    const log_losses = log(at_logits);\n    let loss = log_losses.sum(-1).neg();\n    loss = loss.div(B);\n    return loss;\n  }\n}\n/**\n * Mean Squared Error Loss class, returns the loss given the network output and the expected output.\n */\nexport class MSELoss extends Module {\n  /**\n   * Constructor.\n   */\n  constructor() {\n    super();\n  }\n\n  /**\n   * Performs forward pass through MSELoss, returns loss.\n   * @param {Tensor} z - Output from the last layer of the network.\n   * @param {object} y - Correct outputs expected from the model.\n   * @returns {object} Mean Squared Error loss of the model output.\n   */\n  forward(z: Tensor, y: Tensor): Tensor {\n    // Get data's shape:\n    let zDims = z.shape;\n    // Get last dimension:\n    const D = zDims.slice(zDims.length - 1, zDims.length)[0];\n    // Get product of all batch dimensions:\n    zDims = zDims.slice(0, zDims.length - 1);\n    const B = zDims.reduce((a, b) => a * b, 1);\n    // Flatten out the batch dimensions:\n    z = z.reshape([B, D]);\n    y = y.reshape([B, D]);\n    const S = z.sub(y);\n    const P = S.pow(2);\n    const Su = P.sum();\n    let loss = Su.mean();\n    loss = loss.div(B);\n    return loss;\n  }\n}\n\n/**\n * Saves the model to a JSON file.\n * @param {Module} model - Model to be saved in JSON file.\n * @param {string} file - JSON file.\n */\nexport function save(model: Module, file: string) {\n  /**\n   * Filters object, returning 'null' instead of 'value' for certain keys.\n   * @param {object} obj - Objects with keys and values that we have to filter.\n   * @returns {object} Filtered object.\n   */\n  function recursiveReplacer(obj: { [key: string]: any }): {\n    [key: string]: any;\n  } {\n    let result: { [key: string]: any } = {};\n    for (var x in obj) {\n      if (\n        x !== \"forwardKernel\" &&\n        x !== \"backwardKernelA\" &&\n        x !== \"backwardKernelB\" &&\n        x !== \"gpu\"\n      ) {\n        if (typeof obj[x] === \"object\" && !Array.isArray(obj[x])) {\n          result[x] = recursiveReplacer(obj[x]);\n        } else {\n          result[x] = obj[x];\n        }\n      } else {\n        result[x] = null;\n      }\n    }\n    return result;\n  }\n  const data = JSON.stringify(recursiveReplacer(model));\n  fs.writeFileSync(file, data);\n}\n\n/**\n * Loads a model from a JSON file.\n * @param {Module} model - Blank model to load weights into (placeholder). Needs to be identical to model.\n * @param {string} file - JSON file.\n * @returns {Module} loadedModel - Model to be loaded from JSON file.\n */\nexport function load(model: Module, file: string): Module {\n  const loadedData = fs.readFileSync(file);\n  let loadedModel = JSON.parse(loadedData.toString());\n  loadParameters(loadedModel, model);\n  return model;\n}\n\nfunction loadParameters(source: Module, target: Module) {\n  for (const [key, value] of target.entries()) {\n    // Add every Module, Parameter or Tensor with requires_grad set to True:\n    if (value instanceof Module) {\n      loadParameters(source[key], target[key]);\n    } else if (value instanceof Parameter || value instanceof Tensor) {\n      target[key]._data = source[key]._data;\n      target[key].m = source[key].m;\n      target[key].v = source[key].v;\n    }\n  }\n}\n","import { Parameter, Tensor, zeros } from \"./tensor\";\n\nexport class Adam {\n  // Declare Adam's types:\n  params: (Parameter | Tensor)[];\n  lr: number;\n  reg: number;\n  b1: number;\n  b2: number;\n  eps: number;\n\n  /**\n   * Adam optimizer class.\n   * @param {(Parameter | Tensor)[]} params - List of all Parameter or Tensor (with requires_grad = True) to be optimized by Adam. \"params\" is usually set to nn.Module.parameters(), which automatically returns all parameters in a list form.\n   * @param {number} lr - Scalar multiplying each learning step, controls speed of learning.\n   * @param {number} reg - Scalar controling strength l2 regularization.\n   * @param {(number)[]} betas - Two scalar floats controling how slowly the optimizer changes the \"m\" and \"v\" attributes.\n   * @param {number} eps - Scalar added to denominator to stop it from ever going to zero.\n   */\n  constructor(\n    params: (Parameter | Tensor)[],\n    lr = 1e-3,\n    reg = 0,\n    betas = [0.9, 0.99],\n    eps = 1e-9\n  ) {\n    this.params = params;\n    this.lr = lr;\n    this.reg = reg;\n    this.b1 = betas[0];\n    this.b2 = betas[1];\n    this.eps = eps;\n    this.reg = reg;\n    // Initialize momentum and velocity cumulatives for every parameter:\n    for (let i = 0; i < this.params.length; i++) {\n      this.params[i].m = zeros(this.params[i].shape);\n      this.params[i].v = zeros(this.params[i].shape);\n    }\n  }\n\n  /**\n   * Updates all parameters in this.params with their gradients.\n   */\n  step() {\n    for (let i = 0; i < this.params.length; i++) {\n      this.params[i].m = this.params[i].m\n        ?.mul(this.b1)\n        .add(this.params[i]._grad?.mul(1 - this.b1));\n      this.params[i].v = this.params[i].v\n        ?.mul(this.b2)\n        .add(this.params[i]._grad?.pow(2).mul(1 - this.b2));\n\n      const update_tensor = this.params[i].m\n        ?.mul(this.lr)\n        .div(this.params[i].v?.sqrt().add(this.eps))\n        .neg();\n      const regularization_tensor = this.params[i]\n        .mul(this.reg * this.lr)\n        .neg();\n\n      this.params[i]._data = this.params[i].add(\n        update_tensor?.add(regularization_tensor)\n      )._data;\n    }\n  }\n\n  /**\n   * Sets all the gradients of self.params to zero.\n   */\n  zero_grad() {\n    for (let i = 0; i < this.params.length; i++) {\n      this.params[i].zero_grad();\n    }\n  }\n}\n","import {\n  Tensor,\n  Parameter,\n  add,\n  neg,\n  mul,\n  div,\n  matmul,\n  exp,\n  log,\n  sqrt,\n  pow,\n  mean,\n  masked_fill,\n  variance,\n  at,\n  reshape,\n  _reshape,\n  transpose,\n  tensor,\n  randint,\n  randn,\n  rand,\n  tril,\n  ones,\n  zeros,\n  broadcast\n} from \"./tensor.js\";\nimport {\n  Module,\n  Linear,\n  MultiHeadSelfAttention,\n  FullyConnected,\n  Block,\n  Embedding,\n  PositionalEmbedding,\n  ReLU,\n  Softmax,\n  Dropout,\n  LayerNorm,\n  CrossEntropyLoss,\n  MSELoss,\n  save,\n  load\n} from \"./layers.js\";\nimport { Adam } from \"./optim.js\";\nimport { getShape } from \"./utils.js\";\n\nconst nn = {\n  Module,\n  Linear,\n  MultiHeadSelfAttention,\n  FullyConnected,\n  Block,\n  Embedding,\n  PositionalEmbedding,\n  ReLU,\n  Softmax,\n  Dropout,\n  LayerNorm,\n  CrossEntropyLoss,\n  MSELoss\n};\n\nconst optim = { Adam };\n\nexport const torch = {\n  // Add methods from tensor.js (these methods are accessed with \"torch.\"):\n  Tensor,\n  Parameter,\n  add,\n  neg,\n  mul,\n  div,\n  matmul,\n  exp,\n  log,\n  sqrt,\n  pow,\n  mean,\n  masked_fill,\n  variance,\n  at,\n  reshape,\n  _reshape,\n  transpose,\n  tensor,\n  randint,\n  randn,\n  rand,\n  tril,\n  ones,\n  zeros,\n  broadcast,\n  save,\n  load,\n  // Add submodules:\n  nn,\n  optim,\n  getShape\n};\n"],"names":[],"mappings":";;;IAGA;;;;;IAKG;aAEa,QAAQ,CACtB,IAAkC,EAClC,QAAuB,EAAE,EAAA;;QAGzB,IAAI,IAAI,YAAY,KAAK,IAAI,IAAI,CAAC,MAAM,KAAK,CAAC,EAAE;YAC9C,OAAO,CAAC,CAAC,CAAC;;;IAIZ,IAAA,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;;YAE5B,IAAI,IAAI,CAAC,SAAS,CAAC,KAAK,CAAC,KAAK,IAAI,EAAE;gBAClC,OAAO,CAAC,CAAC,CAAC;;;IAGZ,QAAA,OAAO,KAAK;;IAGd,IAAA,IAAI,OAAO,IAAI,CAAC,CAAC,CAAC,KAAK,QAAQ,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,EAAE;IACtD,QAAA,KAAK,MAAM,OAAO,IAAI,IAAI,EAAE;IAC1B,YAAA,IAAI,OAAO,OAAO,IAAI,QAAQ,EAAE;IAC9B,gBAAA,MAAM,IAAI,KAAK,CAAC,iDAAiD,CAAC;;;;IAItE,QAAA,KAAK,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC;IACvB,QAAA,OAAO,KAAK;;QAGd,IAAI,KAAK,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAAE;YAC1B,IAAI,aAAa,GAAG,IAAI,CAAC,CAAC,CAAC,CAAC,MAAM;;IAElC,QAAA,KAAK,MAAM,OAAO,IAAI,IAAI,EAAE;;gBAE1B,IAAI,OAAO,OAAO,IAAI,QAAQ,IAAI,OAAO,OAAO,IAAI,QAAQ,EAAE;IAC5D,gBAAA,MAAM,IAAI,KAAK,CAAC,4CAA4C,CAAC;;IACxD,iBAAA,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,IAAI,aAAa,IAAI,OAAO,CAAC,MAAM,EAAE;;IAEpE,gBAAA,MAAM,IAAI,KAAK,CAAC,iDAAiD,CAAC;;IAC7D,iBAAA,IAAI,KAAK,CAAC,OAAO,CAAC,OAAO,CAAC,EAAE;IACjC,gBAAA,aAAa,GAAG,OAAO,CAAC,MAAM;;;IAGlC,QAAA,KAAK,CAAC,IAAI,CAAC,IAAI,CAAC,MAAM,CAAC;;QAEzB,OAAO,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC;IACjC;IAEA;;;;IAIG;IACG,SAAU,WAAW,CAAC,CAAkC,EAAA;IAC5D,IAAA,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;IACpB,QAAA,OAAO,CAAC;;IACH,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAChC,OAAO,CAAC,CAAC,CAAC;;IACL,SAAA,IAAI,CAAC,KAAK,IAAI,EAAE;IACrB,QAAA,OAAO,CAAC;;QAEV,OAAO,CAAC,CAAC,KAAK;IAChB;IAEA;;;;IAIG;IACG,SAAU,OAAO,CACrB,CAAkC,EAAA;IAElC,IAAA,IAAI,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE;IACpB,QAAA,OAAO,CAAC;;IAEV,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACzB,QAAA,OAAO,CAAC;;QAEV,OAAO,CAAC,CAAC,KAAK;IAChB;;ICxFA;UAEa,MAAM,CAAA;QACV,aAAa,GAAY,KAAK;IAC9B,IAAA,KAAK;IACL,IAAA,KAAK;IACL,IAAA,KAAK;IACL,IAAA,QAAQ;IACR,IAAA,OAAO;IACP,IAAA,SAAS;QACT,OAAO,GAAY,KAAK;IACxB,IAAA,CAAC;IACD,IAAA,CAAC;IACD,IAAA,MAAM;IACN,IAAA,aAAa;IACb,IAAA,eAAe;IACf,IAAA,eAAe;IACf,IAAA,UAAU;IACV,IAAA,GAAG;IACH,IAAA,MAAM;IAEb;;;;;IAKG;QACH,WACE,CAAA,IAAyB,EACzB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;IAEd,QAAA,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;IAC5B,YAAA,IAAI,CAAC,KAAK,GAAG,IAAI;;IACZ,aAAA,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;IACnC,YAAA,IAAI,CAAC,KAAK,GAAG,CAAC,IAAI,CAAC;;iBACd;IACL,YAAA,MAAM,KAAK,CAAC,sDAAsD,CAAC;;IAErE,QAAA,IAAI,CAAC,KAAK,GAAG,QAAQ,CAAC,IAAI,CAAC;IAC3B,QAAA,IAAI,CAAC,MAAM,GAAG,MAAM;IACpB,QAAA,IAAI,CAAC,aAAa,GAAG,aAAa;IAClC,QAAA,IAAI,CAAC,aAAa,GAAG,IAAI;IACzB,QAAA,IAAI,CAAC,UAAU,GAAG,IAAI;IACtB,QAAA,IAAI,CAAC,GAAG,GAAG,IAAI;IACf,QAAA,IAAI,CAAC,MAAM,GAAG,KAAK;;IAGnB,QAAA,IAAI,IAAI,CAAC,aAAa,EAAE;gBACtB,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC;;;IAIhC,QAAA,IAAI,CAAC,QAAQ,GAAG,EAAE;IAClB,QAAA,IAAI,CAAC,OAAO,GAAG,EAAE;IACjB,QAAA,IAAI,CAAC,SAAS,GAAG,IAAI;IACrB,QAAA,IAAI,CAAC,OAAO,GAAG,KAAK;;IAGtB;;IAEG;IACH,IAAA,IAAI,IAAI,GAAA;YACN,OAAO,IAAI,CAAC,KAAK;;IAGnB;;IAEG;IACH,IAAA,IAAI,MAAM,GAAA;IACR,QAAA,OAAO,IAAI,CAAC,KAAK,CAAC,MAAM;;IAG1B;;IAEG;IACH,IAAA,IAAI,KAAK,GAAA;IACP,QAAA,OAAO,IAAI,CAAC,KAAK,CAAC,MAAM;;IAG1B;;IAEG;IACH,IAAA,IAAI,IAAI,GAAA;IACN,QAAA,OAAO,IAAI,CAAC,KAAK,EAAE,IAAI;;IAGzB;;;IAGG;IACH,IAAA,QAAQ,CAAC,IAAA,GAAsB,IAAI,EAAE,QAAuB,IAAI,EAAA;;IAE9D,QAAA,IAAI,CAAC,IAAI,CAAC,aAAa,EAAE;IACvB,YAAA,MAAM,IAAI,KAAK,CAAC,4CAA4C,CAAC;;;;IAK/D,QAAA,IAAI,IAAI,KAAK,IAAI,EAAE;IACjB,YAAA,IAAI,GAAG,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC;IACvB,YAAA,IAAI,CAAC,QAAQ,GAAG,EAAE;;;IAIpB,QAAA,IAAI,CAAC,KAAK,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC,IAAI,CAAC,CAAC;IAE1D,QAAA,IAAI,KAAK,IAAI,IAAI,EAAE;gBACjB,MAAM,GAAG,GAAG,IAAI,CAAC,QAAQ,CAAC,OAAO,CAAC,KAAK,CAAC;gBACxC,IAAI,CAAC,QAAQ,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,CAAC;;;IAI9B,QAAA,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;;gBAE1B,IAAI,IAAI,CAAC,QAAQ,CAAC,MAAM,KAAK,CAAC,EAAE;oBAC9B,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,IAAI,CAAC,KAAK,EAAE,IAAI,CAAC;;;;IAK/C;;;;;IAKG;IACH,IAAA,EAAE,CAAC,MAAc,EAAA;IACf,QAAA,IAAI,CAAC,MAAM,GAAG,MAAM;;IAGtB;;IAEG;QACH,SAAS,GAAA;YACP,IAAI,CAAC,KAAK,GAAG,KAAK,CAAC,IAAI,CAAC,KAAK,CAAC;IAC9B,QAAA,IAAI,CAAC,QAAQ,GAAG,EAAE;IAClB,QAAA,IAAI,CAAC,OAAO,GAAG,EAAE;IACjB,QAAA,IAAI,CAAC,SAAS,GAAG,IAAI;IACrB,QAAA,IAAI,IAAI,CAAC,CAAC,YAAY,MAAM,IAAI,IAAI,CAAC,CAAC,YAAY,MAAM,EAAE;IACxD,YAAA,IAAI,CAAC,CAAC,CAAC,eAAe,EAAE;IACxB,YAAA,IAAI,CAAC,CAAC,CAAC,eAAe,EAAE;;;IAI5B;;IAEG;QACH,eAAe,GAAA;YACb,IAAI,CAAC,SAAS,EAAE;IAChB,QAAA,IAAI,IAAI,CAAC,SAAS,IAAI,IAAI,EAAE;IAC1B,YAAA,KAAK,MAAM,MAAM,IAAI,IAAI,CAAC,OAAO,EAAE;oBACjC,MAAM,CAAC,eAAe,EAAE;IACxB,gBAAA,MAAM,CAAC,OAAO,GAAG,EAAE;;IAErB,YAAA,IAAI,CAAC,SAAS,GAAG,IAAI;IACrB,YAAA,IAAI,CAAC,OAAO,GAAG,EAAE;IACjB,YAAA,IAAI,CAAC,QAAQ,GAAG,EAAE;;;IAItB;;IAEG;QACH,MAAM,GAAA;YACJ,OAAO,IAAI,CAAC,KAAK;;IAGnB;;;;;IAKG;QACH,GAAG,CAAC,GAAG,GAAG,CAAC,CAAC,EAAE,QAAQ,GAAG,KAAK,EAAA;IAC5B,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;YAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,GAAG,EAAE,QAAQ,CAAC;;IAG/C;;;;;IAKG;QACH,IAAI,CAAC,GAAG,GAAG,CAAC,CAAC,EAAE,QAAQ,GAAG,KAAK,EAAA;IAC7B,QAAA,MAAM,SAAS,GAAG,IAAI,IAAI,EAAE;YAC5B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,GAAG,EAAE,QAAQ,CAAC;;IAG/C;;;;;IAKG;QACH,QAAQ,CAAC,GAAG,GAAG,CAAC,CAAC,EAAE,QAAQ,GAAG,KAAK,EAAA;IACjC,QAAA,MAAM,SAAS,GAAG,IAAI,QAAQ,EAAE;YAChC,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,GAAG,EAAE,QAAQ,CAAC;;IAG/C;;;;IAIG;IACH,IAAA,GAAG,CAAC,KAAsB,EAAA;IACxB,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;YAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,KAAK,CAAC;;IAGvC;;;;IAIG;IACH,IAAA,GAAG,CAAC,KAAsB,EAAA;IACxB,QAAA,IAAI,OAAO,KAAK,KAAK,QAAQ,EAAE;IAC7B,YAAA,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,KAAK,CAAC;;IAClB,aAAA,IAAI,KAAK,YAAY,MAAM,EAAE;gBAClC,OAAO,IAAI,CAAC,GAAG,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC;;iBACvB;IACL,YAAA,MAAM,KAAK,CAAC,+CAA+C,CAAC;;;IAIhE;;;IAGG;QACH,GAAG,GAAA;IACD,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;IAC3B,QAAA,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC;;IAGhC;;;;IAIG;IACH,IAAA,GAAG,CAAC,KAAsB,EAAA;IACxB,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;YAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,KAAK,CAAC;;IAGvC;;;;IAIG;IACH,IAAA,GAAG,CAAC,KAAsB,EAAA;IACxB,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;YAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,KAAK,CAAC;;IAGvC;;;;IAIG;IACH,IAAA,MAAM,CAAC,KAAa,EAAA;IAClB,QAAA,MAAM,SAAS,GAAG,IAAI,MAAM,EAAE;IAC9B,QAAA,IAAI,MAAM;IACV,QAAA,IAAI,IAAI,CAAC,MAAM,KAAK,KAAK,IAAI,KAAK,CAAC,MAAM,KAAK,KAAK,EAAE;gBACnD,MAAM,GAAG,KAAK;;iBACT;gBACL,MAAM,GAAG,KAAK;;;YAGhB,IAAI,KAAK,CAAC,aAAa,KAAK,IAAI,IAAI,KAAK,CAAC,UAAU,IAAI,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE;IACzE,YAAA,IAAI,MAAM,KAAK,KAAK,EAAE;;oBAEpB,MAAM,EAAE,GAAG,EAAE,GAAG,OAAO,CAAC,qBAAqB,CAAC;;IAE9C,gBAAA,IAAI,KAAK,CAAC,UAAU,IAAI,IAAI,EAAE;IAC5B,oBAAA,KAAK,CAAC,UAAU,GAAG,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;IACrC,oBAAA,IAAI,KAAK,CAAC,MAAM,KAAK,KAAK,EAAE;IAC1B,wBAAA,OAAO,CAAC,IAAI,CACV,0GAA0G,CAC3G;IACD,wBAAA,KAAK,CAAC,MAAM,GAAG,IAAI;;;IAGvB,gBAAA,KAAK,CAAC,GAAG,GAAG,IAAI,GAAG,EAAE;;IAErB,gBAAA,MAAM,UAAU,GAAG,UAEjB,CAAa,EACb,CAAa,EACb,GAAW,EAAA;wBAEX,IAAI,GAAG,GAAG,CAAC;IACX,oBAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,GAAG,EAAE,CAAC,EAAE,EAAE;4BAC5B,GAAG,IAAI,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC;;IAElD,oBAAA,OAAO,GAAG;IACZ,iBAAC;;IAED,gBAAA,KAAK,CAAC,aAAa,GAAG,KAAK,CAAC;IACzB,qBAAA,YAAY,CAAC,UAAU,EAAE,EAAE,iBAAiB,EAAE,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE;yBAClE,SAAS,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACrD,gBAAA,KAAK,CAAC,eAAe,GAAG,KAAK,CAAC;IAC3B,qBAAA,YAAY,CAAC,UAAU,EAAE,EAAE,iBAAiB,EAAE,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE;yBAClE,SAAS,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;IACpD,gBAAA,KAAK,CAAC,eAAe,GAAG,KAAK,CAAC;IAC3B,qBAAA,YAAY,CAAC,UAAU,EAAE,EAAE,iBAAiB,EAAE,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE;yBACjE,SAAS,CAAC,CAAC,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,EAAE,KAAK,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;qBACjD;;IAEL,gBAAA,MAAM,UAAU,GAAG,UACjB,CAAa,EACb,CAAa,EACb,GAAW,EAAA;IAEX,oBAAA,MAAM,GAAG,GAAG,KAAK,CAAC,CAAC,CAAC,MAAM;6BACvB,IAAI,CAAC,CAAC;IACN,yBAAA,GAAG,CAAC,MAAM,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IACxC,oBAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACjC,wBAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;gCACpC,IAAI,YAAY,GAAG,CAAC;IACpB,4BAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,GAAG,EAAE,CAAC,EAAE,EAAE;IAC5B,gCAAA,YAAY,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;gCAEnC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,YAAY;;;IAG5B,oBAAA,OAAO,GAAG;IACZ,iBAAC;;IAED,gBAAA,KAAK,CAAC,aAAa,GAAG,UAAU;IAChC,gBAAA,KAAK,CAAC,eAAe,GAAG,UAAU;IAClC,gBAAA,KAAK,CAAC,eAAe,GAAG,UAAU;;;;IAItC,QAAA,KAAK,CAAC,UAAU,GAAG,IAAI,CAAC,KAAK,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;YACpC,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,KAAK,CAAC;;IAGvC;;;;IAIG;IACH,IAAA,GAAG,CAAC,CAAS,EAAA;IACX,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;YAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,CAAC,CAAC;;IAGnC;;;IAGG;QACH,IAAI,GAAA;IACF,QAAA,MAAM,SAAS,GAAG,IAAI,IAAI,EAAE;IAC5B,QAAA,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC;;IAGhC;;;IAGG;QACH,GAAG,GAAA;IACD,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;IAC3B,QAAA,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC;;IAGhC;;;IAGG;QACH,GAAG,GAAA;IACD,QAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;IAC3B,QAAA,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,CAAC;;IAGhC;;;;;IAKG;QACH,SAAS,CAAC,IAAY,EAAE,IAAY,EAAA;IAClC,QAAA,MAAM,SAAS,GAAG,IAAI,SAAS,EAAE;YACjC,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,IAAI,EAAE,IAAI,CAAC;;IAG5C;;;;;;;;;;;;;;;;IAgBG;QACH,EAAE,CAAC,MAA2B,EAAE,MAA4B,EAAA;IAC1D,QAAA,MAAM,SAAS,GAAG,IAAI,EAAE,EAAE;YAC1B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,MAAM,EAAE,MAAM,CAAC;;IAGhD;;;;;;;;;;;;;IAaG;IACH,IAAA,WAAW,CACT,IAAY,EACZ,SAAuC,EACvC,KAAa,EAAA;IAEb,QAAA,MAAM,SAAS,GAAG,IAAI,UAAU,EAAE;IAClC,QAAA,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,IAAI,EAAE,SAAS,EAAE,KAAK,CAAC;;IAGxD;;;;IAIG;IACH,IAAA,OAAO,CAAC,KAAoB,EAAA;IAC1B,QAAA,MAAM,SAAS,GAAG,IAAI,OAAO,EAAE;YAC/B,OAAO,SAAS,CAAC,OAAO,CAAC,IAAI,EAAE,KAAK,CAAC;;IAExC;IAED;IAEM,MAAO,SAAU,SAAQ,MAAM,CAAA;IACnC;;;IAGG;IACH,IAAA,WAAA,CAAY,IAAyB,EAAA;IACnC,QAAA,KAAK,CAAC,IAAI,EAAE,IAAI,CAAC;;IAEpB;IAED;UAEa,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;;IAKG;QACH,OAAO,CAAC,CAA2B,EAAE,CAA2B,EAAA;;YAE9D,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;IAEnB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;IACxB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;;IAGxB,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,KAAK,EAAE,KAAK,CAAC;YAClB,YAAY,CAAC,CAAC,CAAC,IAAI,YAAY,CAAC,CAAC,CAAC;aACnC;;YAGD,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;YAEpB,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;IAEpB,QAAA,CAAC,CAAC,SAAS,GAAG,IAAI;IAElB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGzB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;gBACnB,IAAI,EAAE,GAAG,EAAE;;IAEX,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAInB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;gBACnB,IAAI,EAAE,GAAG,EAAE;;IAEX,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;;IAEf,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC;IACb,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;IAEpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,MAAM,EAAE,GAAG,GAAG,CAAC,EAAE,CAAC;IAClB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;;IAKG;QACH,OAAO,CAAC,CAAkB,EAAE,CAAkB,EAAA;;YAE5C,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;IAEnB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;IACxB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;;IAGxB,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,KAAK,EAAE,KAAK,CAAC;YAClB,YAAY,CAAC,CAAC,CAAC,IAAI,YAAY,CAAC,CAAC,CAAC;aACnC;;YAGD,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;YAEpB,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;IAEpB,QAAA,CAAC,CAAC,SAAS,GAAG,IAAI;IAElB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGzB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;;IAE9C,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAInB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;;IAE9C,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;;IAKG;QACH,OAAO,CAAC,CAAS,EAAE,CAAkB,EAAA;;YAEnC,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;IAEnB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;IACxB,QAAA,MAAM,KAAK,GAAG,OAAO,CAAC,CAAC,CAAC;;IAGxB,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,KAAK,EAAE,KAAK,CAAC;YAClB,YAAY,CAAC,CAAC,CAAC,IAAI,YAAY,CAAC,CAAC,CAAC;aACnC;;YAGD,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;YAEpB,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;IAEpB,QAAA,CAAC,CAAC,SAAS,GAAG,IAAI;IAElB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGzB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;gBAEnB,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;IAGvD,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IAErB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAInB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CACjB,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAC3D;;IAED,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IAErB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;IAED,MAAM,MAAM,CAAA;IACV,IAAA,KAAK;IACL,IAAA,UAAU;IACV,IAAA,MAAM;QACN,OAAO,CAAC,CAAS,EAAE,CAAS,EAAA;YAC1B,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,CAAC,CAAC;IACnB,QAAA,IAAI,KAAK,GAAG,CAAC,CAAC,IAAI;IAClB,QAAA,IAAI,KAAK,GAAG,CAAC,CAAC,IAAI;IAElB,QAAA,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,EAAE;IACnC,YAAA,KAAK,GAAG,WAAW,CAAC,KAAK,EAAE,KAAK,CAAC;;iBAC5B;IACL,YAAA,KAAK,GAAG,WAAW,CAAC,KAAK,EAAE,KAAK,CAAC;;IAEnC,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,OAAO,CAAC,KAAK,EAAE,KAAK,EAAE,CAAC,CAAC,aAAa,CAAC;;IAEtC,QAAA,YAAY,CAAC,CAAC,CAAC,IAAI,YAAY,CAAC,CAAC;;aAElC;YACD,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;YAEpB,IAAI,CAAC,YAAY,MAAM,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IAC1C,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;;IAEpB,QAAA,CAAC,CAAC,SAAS,GAAG,IAAI;IAClB,QAAA,OAAO,CAAC;;QAEV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;YAC5B,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,IAAI,CAAC,KAAK;IACzB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,MAAM,MAAM,GAAG,EAAE,CAAC,IAAI;IACtB,YAAA,IAAI,GAAG,GAAG,UAAU,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,GAAG,CAAC,CAAC;IACzC,YAAA,GAAG,GAAG,WAAW,CAAC,GAAG,EAAE,MAAM,CAAC;IAC9B,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,CAAC,MAAM,EAAE,GAAG,EAAE,CAAC,CAAC,eAAe,CAAC,CAAC;IAC5D,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;IAEnB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,MAAM,MAAM,GAAG,EAAE,CAAC,IAAI;IACtB,YAAA,IAAI,GAAG,GAAG,UAAU,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,GAAG,CAAC,CAAC;IACzC,YAAA,GAAG,GAAG,WAAW,CAAC,GAAG,EAAE,MAAM,CAAC;IAC9B,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,CAAC,GAAG,EAAE,MAAM,EAAE,CAAC,CAAC,eAAe,CAAC,CAAC;IAC5D,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;;IAKG;QACH,OAAO,CAAC,CAAS,EAAE,CAAS,EAAA;;IAE1B,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC;IACnB,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;gBAEnB,MAAM,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,IAAI,EAAE,EAAE,CAAC,IAAI,CAAC,CAAC,CAAC;IACrD,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,IAAI,CAAA;IACf,IAAA,KAAK;IAEL;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;;IAEf,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC;IACd,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,MAAM,EAAE,GAAG,IAAI,MAAM,CACnB,IAAI,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,EAAE,KAAK,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,IAAI,CAAC,CACxD;IACD,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IACL;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;;IAEf,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC;IACb,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,MAAM,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC,IAAI,CAAC,EAAE,EAAE,CAAC,IAAI,CAAC,CAAC;IAClD,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;;IAEf,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC;IACb,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;gBAEnB,MAAM,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,IAAI,CAAC,EAAE,EAAE,CAAC,IAAI,CAAC,CAAC;IAErD,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;IAED;UAEa,GAAG,CAAA;IACd,IAAA,KAAK;IAEL;;;;;;IAMG;IACH,IAAA,OAAO,CAAC,CAAS,EAAE,GAAW,EAAE,QAAQ,GAAG,KAAK,EAAA;;YAE9C,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,GAAG,EAAE,QAAQ,CAAC;;IAG/B,QAAA,IAAI,GAAG,GAAG,CAAC,EAAE;gBACX,GAAG,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,GAAG;;;YAG5B,IAAI,GAAG,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,EAAE;IACzB,YAAA,MAAM,KAAK,CAAC,8BAA8B,CAAC;;;IAG7C,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,IAAI,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,EAAE,QAAQ,CAAC;IAC5B,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,GAAG,EAAE,QAAQ,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGrC,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;gBACnB,IAAI,QAAQ,EAAE;IACZ,gBAAA,EAAE,GAAG,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC;;gBAGlB,MAAM,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IAE3B,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,IAAI,CAAA;IACf,IAAA,KAAK;IAEL;;;;;;IAMG;IACH,IAAA,OAAO,CAAC,CAAS,EAAE,GAAW,EAAE,QAAQ,GAAG,KAAK,EAAA;;IAE9C,QAAA,IAAI,GAAG,GAAG,CAAC,EAAE;gBACX,GAAG,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,GAAG;;;YAG5B,IAAI,GAAG,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,EAAE;IACzB,YAAA,MAAM,KAAK,CAAC,8BAA8B,CAAC;;;YAI7C,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC;;IAGrB,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,KAAK,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,EAAE,QAAQ,CAAC;IAC7B,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,IAAI,CAAC,KAAK;;IAG3B,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,IAAI,EAAE,GAAG,IAAI,MAAM,CAAC,IAAI,CAAC,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC;;IAEhD,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;IACrB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,QAAQ,CAAA;IACnB,IAAA,KAAK;IACL;;;;;;IAMG;IACH,IAAA,OAAO,CAAC,CAAS,EAAE,GAAW,EAAE,QAAQ,GAAG,KAAK,EAAA;;IAE9C,QAAA,IAAI,GAAG,GAAG,CAAC,EAAE;gBACX,GAAG,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,GAAG;;;YAG5B,IAAI,GAAG,IAAI,CAAC,CAAC,KAAK,CAAC,MAAM,EAAE;IACzB,YAAA,MAAM,KAAK,CAAC,8BAA8B,CAAC;;;YAI7C,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,GAAG,CAAC;;IAGrB,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,SAAS,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,EAAE,QAAQ,CAAC;IACjC,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,GAAG,CAAC,GAAG,IAAI,CAAC,KAAK;;IAE3B,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,EAAE,GAAG,SAAS,CAAC,EAAE,EAAE,CAAC,CAAC;;gBAErB,MAAM,GAAG,GAAG,IAAI,CAAC,CAAC,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,EAAE,IAAI,CAAC,CAAC,CAAC;IAC1D,YAAA,MAAM,OAAO,GAAG,IAAI,CAAC,IAAI,CAAC,EAAE,CAAC,KAAK,EAAE,CAAC,CAAC,EAAE,GAAG,CAAC;IAC5C,YAAA,IAAI,EAAE,GAAG,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC;;IAEpC,YAAA,EAAE,GAAG,IAAI,MAAM,CAAC,EAAE,CAAC;IACnB,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;IAED;UAEa,SAAS,CAAA;IACpB,IAAA,KAAK;IAEL;;;;;;IAMG;IACH,IAAA,OAAO,CAAC,CAAS,EAAE,IAAY,EAAE,IAAY,EAAA;;YAE3C,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC;;IAG5B,QAAA,IAAI,IAAI,GAAG,CAAC,EAAE;gBACZ,IAAI,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,IAAI;;IAE9B,QAAA,IAAI,IAAI,GAAG,CAAC,EAAE;gBACZ,IAAI,GAAG,CAAC,CAAC,KAAK,CAAC,MAAM,GAAG,IAAI;;;IAG9B,QAAA,IAAI,GAAW;IACf,QAAA,IAAI,IAAI,GAAG,IAAI,EAAE;gBACf,GAAG,GAAG,IAAI;;IACL,aAAA,IAAI,IAAI,GAAG,IAAI,EAAE;gBACtB,GAAG,GAAG,IAAI;;iBACL;IACL,YAAA,MAAM,IAAI,KAAK,CAAC,6CAA6C,CAAC;;;IAIhE,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,UAAU,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,CAAC;IACxB,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGlC,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;gBACnB,MAAM,EAAE,GAAG,EAAE,CAAC,SAAS,CAAC,IAAI,EAAE,IAAI,CAAC;IACnC,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,EAAE,CAAA;IACb,IAAA,KAAK;IAEL,IAAA,OAAO,CACL,CAAS,EACT,IAAyB,EACzB,OAAmC,IAAI,EAAA;;YAGvC,IAAI,IAAI,EAAE;gBACR,IAAI,GAAG,WAAW,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC;;YAEzC,IAAI,IAAI,EAAE;gBACR,IAAI,GAAG,WAAW,CAAC,IAAI,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC;;;YAIzC,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC;;IAG5B,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,GAAG,CAAC,CAAC,CAAC,KAAK,EAAE,IAAI,EAAE,IAAI,CAAC;IACxB,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,IAAI,EAAE,IAAI,CAAC,GAAG,IAAI,CAAC,KAAK;;IAElC,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;gBACnB,MAAM,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC;;IAEzB,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,EAAE,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;;IAElC,gBAAA,IAAI,IAAI,IAAI,IAAI,EAAE;IAChB,oBAAA,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,GAAG,IAAI,CAC/B,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAC1B,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CACZ;;;yBAEI;IACL,oBAAA,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,EAAE,EAAE,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;;;IAG5D,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,UAAU,CAAA;IACrB,IAAA,KAAK;IAEL,IAAA,OAAO,CACL,CAAS,EACT,IAAY,EACZ,SAAuC,EACvC,KAAa,EAAA;;YAGb,IAAI,CAAC,KAAK,GAAG,CAAC,CAAC,EAAE,IAAI,EAAE,SAAS,CAAC;;YAGjC,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,YAAY,CAAC,CAAC,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAE,SAAS,EAAE,KAAK,CAAC;IACnD,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;YAE5B,MAAM,CAAC,CAAC,EAAE,IAAI,EAAE,SAAS,CAAC,GAAG,IAAI,CAAC,KAAK;;IAEvC,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;gBAEnB,MAAM,EAAE,GAAG,IAAI,MAAM,CAAC,YAAY,CAAC,EAAE,CAAC,KAAK,EAAE,IAAI,CAAC,KAAK,EAAE,SAAS,EAAE,CAAC,CAAC,CAAC;IAEvE,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;UAEY,OAAO,CAAA;IAClB,IAAA,KAAK;QAEL,OAAO,CAAC,CAAS,EAAE,KAAoB,EAAA;;IAErC,QAAA,IAAI,CAAC,KAAK,GAAG,CAAC;;IAGd,QAAA,MAAM,CAAC,GAAG,IAAI,MAAM,CAClB,QAAQ,CAAC,CAAC,CAAC,KAAK,EAAE,KAAK,CAAC;IACxB,QAAA,YAAY,CAAC,CAAC,CAAC;aAChB;;IAGD,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;IACnB,YAAA,CAAC,CAAC,OAAO,CAAC,IAAI,CAAC,CAAC,CAAC;IACjB,YAAA,CAAC,CAAC,QAAQ,CAAC,IAAI,CAAC,CAAC,CAAC;IAClB,YAAA,CAAC,CAAC,SAAS,GAAG,IAAI;;IAGpB,QAAA,OAAO,CAAC;;QAGV,QAAQ,CAAC,EAAU,EAAE,CAAS,EAAA;;IAE5B,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK;;IAGpB,QAAA,IAAI,YAAY,CAAC,CAAC,CAAC,EAAE;;IAEnB,YAAA,MAAM,EAAE,GAAG,IAAI,MAAM,CAAC,QAAQ,CAAC,EAAE,CAAC,IAAI,EAAE,CAAC,CAAC,KAAK,CAAC,CAAC;IACjD,YAAA,CAAC,CAAC,QAAQ,CAAC,EAAE,EAAE,CAAC,CAAC;;;IAGtB;IAeD;;;;;;IAMG;IACa,SAAA,IAAI,CAAC,CAAS,EAAE,GAAG,GAAG,CAAC,CAAC,EAAE,QAAQ,GAAG,KAAK,EAAA;QACxD,OAAO,CAAC,CAAC,IAAI,CAAC,GAAG,EAAE,QAAQ,CAAC;IAC9B;IAEA;;;;;;IAMG;IACa,SAAA,QAAQ,CAAC,CAAS,EAAE,GAAG,GAAG,CAAC,CAAC,EAAE,QAAQ,GAAG,KAAK,EAAA;QAC5D,OAAO,CAAC,CAAC,QAAQ,CAAC,GAAG,EAAE,QAAQ,CAAC;IAClC;IAEA;;;;;IAKG;IACa,SAAA,GAAG,CAAC,CAAS,EAAE,CAAkB,EAAA;IAC/C,IAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACjB;IAYA;;;IAGG;IACG,SAAU,GAAG,CAAC,CAAS,EAAA;IAC3B,IAAA,OAAO,CAAC,CAAC,GAAG,EAAE;IAChB;IAEA;;;;IAIG;IACa,SAAA,GAAG,CAAC,CAAS,EAAE,CAAkB,EAAA;IAC/C,IAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IACjB;IAEA;;;;IAIG;IACa,SAAA,GAAG,CAAC,CAAS,EAAE,CAAkB,EAAA;IAC/C,IAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;QAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC;IAChC;IAEA;;;;;IAKG;IACa,SAAA,GAAG,CAAC,CAAS,EAAE,CAAS,EAAA;IACtC,IAAA,MAAM,SAAS,GAAG,IAAI,GAAG,EAAE;QAC3B,OAAO,SAAS,CAAC,OAAO,CAAC,CAAC,EAAE,CAAC,CAAC;IAChC;IAEA;;;;IAIG;IACG,SAAU,IAAI,CAAC,CAAS,EAAA;IAC5B,IAAA,OAAO,CAAC,CAAC,IAAI,EAAE;IACjB;IAEA;;;;IAIG;IACG,SAAU,GAAG,CAAC,CAAS,EAAA;IAC3B,IAAA,OAAO,CAAC,CAAC,GAAG,EAAE;IAChB;IAEA;;;;IAIG;IACG,SAAU,GAAG,CAAC,CAAS,EAAA;IAC3B,IAAA,OAAO,CAAC,CAAC,GAAG,EAAE;IAChB;IAEA;;;;IAIG;IACa,SAAA,MAAM,CAAC,CAAS,EAAE,CAAS,EAAA;IACzC,IAAA,OAAO,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC;IACpB;IAEA;;;;;;IAMG;aACa,SAAS,CAAC,CAAS,EAAE,IAAY,EAAE,IAAY,EAAA;QAC7D,OAAO,CAAC,CAAC,SAAS,CAAC,IAAI,EAAE,IAAI,CAAC;IAChC;IAEA;;;;;;;;;;;;;;;;;IAiBG;aACa,EAAE,CAChB,CAAS,EACT,IAAyB,EACzB,IAAyB,EAAA;QAEzB,OAAO,CAAC,CAAC,EAAE,CAAC,IAAI,EAAE,IAAI,CAAC;IACzB;IAEA;;;;;;;;;;;;;;IAcG;IACG,SAAU,WAAW,CACzB,CAAS,EACT,IAAY,EACZ,SAAuC,EACvC,KAAa,EAAA;QAEb,OAAO,CAAC,CAAC,WAAW,CAAC,IAAI,EAAE,SAAS,EAAE,KAAK,CAAC;IAC9C;IAEA;;;;;IAKG;IACa,SAAA,OAAO,CAAC,CAAS,EAAE,KAAiB,EAAA;IAClD,IAAA,OAAO,CAAC,CAAC,OAAO,CAAC,KAAK,CAAC;IACzB;IAEA;IAEA,SAAS,IAAI,CAAC,CAAa,EAAE,GAAW,EAAE,QAAkB,EAAA;;;;IAI1D,IAAA,IAAI,GAAG,IAAI,CAAC,EAAE;YACZ,MAAM,GAAG,GAAG,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC;YAC7C,IAAI,QAAQ,EAAE;gBACZ,OAAO,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC;;iBAC3B;IACL,YAAA,OAAO,GAAG;;;IAEP,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAChC,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,GAAG,GAAG,CAAC,EAAE,QAAQ,CAAC,CAAC;;aACtD;IACL,QAAA,MAAM,KAAK,CAAC,oBAAoB,CAAC;;IAErC;IAEA,SAAS,KAAK,CAAC,CAAa,EAAE,GAAW,EAAE,QAAkB,EAAA;;;;IAI3D,IAAA,IAAI,GAAG,IAAI,CAAC,EAAE;IACZ,QAAA,MAAM,OAAO,GAAG,IAAI,CAClB,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EACjC,CAAC,CAAC,MAAM,CACT;YACD,IAAI,QAAQ,EAAE;gBACZ,OAAO,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,OAAO,CAAC;;iBAC/B;IACL,YAAA,OAAO,OAAO;;;IAEX,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAChC,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,KAAK,CAAC,OAAO,EAAE,GAAG,GAAG,CAAC,gBAAgB,CAAC;;aAC5D;IACL,QAAA,MAAM,KAAK,CAAC,oBAAoB,CAAC;;IAErC;IAEA,SAAS,SAAS,CAAC,CAAa,EAAE,GAAW,EAAE,QAAkB,EAAA;;;;IAI/D,IAAA,IAAI,GAAG,IAAI,CAAC,EAAE;;IAEZ,QAAA,MAAM,IAAI,GAAG,IAAI,CACf,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EACjC,CAAC,CAAC,MAAM,CACT;;IAED,QAAA,MAAM,OAAO,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,EAAE,KAAK,CAAC,EAAE,GAAG,IAAI,KAAK,CAAC,CAAC;;IAE/C,QAAA,MAAM,QAAQ,GAAG,IAAI,CACnB,OAAO,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC,EACvC,CAAC,CAAC,MAAM,CACT;YACD,IAAI,QAAQ,EAAE;gBACZ,OAAO,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC;;iBAChC;IACL,YAAA,OAAO,QAAQ;;;IAEZ,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAChC,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,SAAS,CAAC,OAAO,EAAE,GAAG,GAAG,CAAC,cAAc,CAAC;;aAC9D;IACL,QAAA,MAAM,KAAK,CAAC,oBAAoB,CAAC;;IAErC;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAE,CAAsB,EAAA;;QAE1D,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAClD,OAAO,CAAC,GAAG,CAAC;;aACP,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,CAAC,YAAY,KAAK,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,CAAC,YAAY,KAAK,EAAE;;IAEnD,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;IAC1B,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;;IAE1B,QAAA,IAAI,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EAAE;gBACrD,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;iBAEhD,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAY;;IAEhB,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAIX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;;;iBAGxC,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAY;;IAEhB,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAIX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE,OAAO,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;;;iBAExC;IACL,YAAA,MAAM,KAAK,CAAC,kCAAkC,CAAC;;;aAE5C;IACL,QAAA,MAAM,KAAK,CAAC,kCAAkC,CAAC;;IAEnD;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAA;;IAElC,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YACzB,OAAO,CAAC,CAAC;;IACJ,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IAChC,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,CAAC,CAAC;;aACnC;IACL,QAAA,MAAM,IAAI,SAAS,CAAC,iCAAiC,CAAC;;IAE1D;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAE,CAAsB,EAAA;;QAE1D,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAClD,OAAO,CAAC,GAAG,CAAC;;aACP,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,CAAC,YAAY,KAAK,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,CAAC,YAAY,KAAK,EAAE;;IAEnD,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;IAC1B,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;;IAE1B,QAAA,IAAI,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EAAE;gBACrD,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;iBAEhD,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAG;;IAEP,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAIX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;;;iBAGxC,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAG;;IAEP,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAIX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE,OAAO,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;;;;IAInD;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAE,CAAsB,EAAA;;QAE1D,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAClD,OAAO,CAAC,GAAG,CAAC;;aACP,IAAI,OAAO,CAAC,KAAK,QAAQ,IAAI,CAAC,YAAY,KAAK,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACtD,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;aACtC,IAAI,CAAC,YAAY,KAAK,IAAI,CAAC,YAAY,KAAK,EAAE;;IAEnD,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;IAC1B,QAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;;IAE1B,QAAA,IAAI,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EAAE;gBACrD,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;iBAEhD,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAG;;IAEP,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAKX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC;;;;iBAGxC,IAAI,MAAM,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACxC,YAAA,IAAI,GAAG;;IAEP,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,gBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IAClD,oBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;wBACA,GAAG,GAAG,CAAC;;;;IAIX,YAAA,IAAI,GAAG,KAAK,CAAC,EAAE;oBACb,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,EAAE,OAAO,CAAC,CAAC;;;qBAEhD;IACL,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,IAAI,CAAC,CAAC,EAAE,OAAO,CAAC,CAAC;;;;IAInD;IAEA,SAAS,OAAO,CAAC,CAAa,EAAE,CAAa,EAAE,MAAW,EAAA;IACxD,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACzB,QAAA,MAAM,IAAI,KAAK,CAAC,0CAA0C,CAAC;;;QAG7D,IAAI,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;YAC/B,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAmB,EAAE,GAAW,KAC5C,OAAO,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,EAAE,MAAM,CAAC,CACjC;;;aAEI;;YAEL,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,KAAK,CAAC,CAAC,MAAM,IAAI,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;IAC3D,YAAA,IAAI,GAAG,GAAG,MAAM,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,MAAM,CAAC;IAChC,YAAA,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,EAAY,KAAK,KAAK,CAAC,IAAI,CAAC,EAAE,CAAC,CAAC;IAC/C,YAAA,OAAO,GAAG;;iBACL;gBACL,MAAM,KAAK,CACT,CAA0D,uDAAA,EAAA;AACxD,gBAAA,CAAC,CAAC,MAAM;AACR,gBAAA,CAAC,CAAC,CAAC,CAAC,CAAC;AACN,aAAA,CAAA,KAAA,EAAQ,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAA,CAAE,CACnC;;;IAGP;IACA;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAE,CAAS,EAAA;;QAE7C,IAAI,CAAC,GAAG,CAAC;IACT,IAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,GAAG,CAAC,EAAE,CAAC,EAAE,EAAE;IAC9B,QAAA,CAAC,GAAG,IAAI,CAAC,CAAC,EAAE,CAAC,CAAC;;IAEhB,IAAA,OAAO,CAAC;IACV;IAEA,SAAS,KAAK,CAAC,CAAsB,EAAA;;IAEnC,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACzB,QAAA,OAAO,IAAI,CAAC,IAAI,CAAC,CAAC,CAAC;;IACd,SAAA,IAAI,CAAC,YAAY,KAAK,EAAE;IAC7B,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAmB,KAAK,KAAK,CAAC,OAAO,CAAC,CAAC;;aAChD;IACL,QAAA,MAAM,IAAI,SAAS,CAAC,iCAAiC,CAAC;;IAE1D;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAA;;IAElC,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YACzB,OAAO,iBAAiB,IAAI,CAAC;;IACxB,SAAA,IAAI,CAAC,YAAY,KAAK,EAAE;IAC7B,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAmB,KAAK,IAAI,CAAC,OAAO,CAAC,CAAC;;aAC/C;IACL,QAAA,MAAM,IAAI,SAAS,CAAC,iCAAiC,CAAC;;IAE1D;IAEA,SAAS,IAAI,CAAC,CAAsB,EAAA;;IAElC,IAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACzB,QAAA,OAAO,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;;IACb,SAAA,IAAI,CAAC,YAAY,KAAK,EAAE;IAC7B,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAmB,KAAK,IAAI,CAAC,OAAO,CAAC,CAAC;;aAC/C;IACL,QAAA,MAAM,IAAI,SAAS,CAAC,iCAAiC,CAAC;;IAE1D;IAEA,SAAS,UAAU,CAAC,CAAa,EAAE,GAAW,EAAA;;IAE5C,IAAA,IAAI,GAAG,IAAI,CAAC,EAAE;;YAEZ,MAAM,QAAQ,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM;iBAC/B,IAAI,CAAC,CAAC;IACN,aAAA,GAAG,CAAC,MAAM,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC;IAErC,QAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACjC,YAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACpC,gBAAA,QAAQ,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;;IAG5B,QAAA,OAAO,QAAQ;;IACV,SAAA,IAAI,CAAC,YAAY,KAAK,EAAE;IAC7B,QAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAmB,KAAK,UAAU,CAAC,OAAO,EAAE,GAAG,GAAG,CAAC,CAAC,CAAC;;aAC9D;IACL,QAAA,MAAM,KAAK,CAAC,qCAAqC,CAAC;;IAEtD;IAEA,SAAS,GAAG,CACV,CAAa,EACb,IAAgB,EAChB,IAAuB,EAAA;;QAGvB,IAAI,IAAI,EAAE;IACR,QAAA,OAAO,KAAK,CAAC,IAAI,CAAC,MAAM;iBACrB,IAAI,CAAC,CAAC;iBACN,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;;;aAEhC;IACL,QAAA,OAAO,KAAK,CAAC,IAAI,CAAC,MAAM;iBACrB,IAAI,CAAC,CAAC;IACN,aAAA,GAAG,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,CAAC,CAAC;;IAEhC;IAEA,SAAS,YAAY,CACnB,CAAsB,EACtB,IAAyB,EACzB,SAAuC,EACvC,KAAa,EAAA;;IAGb,IAAA,IAAI,OAAO,IAAI,KAAK,QAAQ,EAAE;IAC5B,QAAA,IAAI,OAAO,CAAC,IAAI,QAAQ,EAAE;IACxB,YAAA,MAAM,IAAI,KAAK,CAAC,mCAAmC,CAAC;;IAEtD,QAAA,IAAI,SAAS,CAAC,IAAI,CAAC,EAAE;IACnB,YAAA,OAAO,KAAK;;iBACP;IACL,YAAA,OAAO,CAAC;;;IAEL,SAAA,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;YAChC,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KACxB,YAAY,CAAC,OAAO,EAAE,IAAI,CAAC,GAAG,CAAC,EAAE,SAAS,EAAE,KAAK,CAAC,CACnD;;aACI;IACL,QAAA,MAAM,IAAI,KAAK,CAAC,iCAAiC,CAAC;;IAEtD;IAEA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IAEA;IACA;IACA;IACA;IACA;IAEgB,SAAA,QAAQ,CAAC,CAAa,EAAE,KAAe,EAAA;IACrD,IAAA,IACE,QAAQ,CAAC,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,IAAI,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC,EAC1E;IACA,QAAA,MAAM,IAAI,KAAK,CAAC,2CAA2C,CAAC;;QAE9D,SAAS,MAAM,CACb,EAAS,EACT,MAAgB,EAChB,GAAW,EACX,WAAmB,EAAA;IAEnB,QAAA,IAAI,MAAM,CAAC,MAAM,GAAG,CAAC,EAAE;IACrB,YAAA,MAAM,UAAU,GAAG,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;gBAC3C,IAAI,MAAM,GAAG,GAAG;IAChB,YAAA,WAAW,GAAG,WAAW,GAAG,MAAM,CAAC,CAAC,CAAC;IACrC,YAAA,MAAM,OAAO,GAAG,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,GAAG,KACpC,MAAM,CAAC,EAAE,EAAE,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,MAAM,GAAG,GAAG,GAAG,WAAW,EAAE,WAAW,CAAC,CACrE;IACD,YAAA,OAAO,OAAO;;iBACT;IACL,YAAA,MAAM,OAAO,GAAG,EAAE,CAAC,KAAK,CAAC,GAAG,EAAE,GAAG,GAAG,WAAW,CAAC;IAChD,YAAA,OAAO,OAAO;;;QAGlB,MAAM,IAAI,GAAG,CAAC,CAAC,IAAI,CAAC,QAAQ,CAAC;IAC7B,IAAA,MAAM,KAAK,GAAG,MAAM,CAAC,IAAI,EAAE,KAAK,EAAE,CAAC,EAAE,IAAI,CAAC,MAAM,CAAC;IACjD,IAAA,OAAO,KAAK;IACd;IAEA;IAEA;;;;;IAKG;IACH,SAAS,kBAAkB,CACzB,KAAoB,EACpB,SAAuB,EAAA;IAEvB,IAAA,IAAI,KAAK,CAAC,MAAM,KAAK,CAAC,EAAE;IACtB,QAAA,MAAM,UAAU,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;YAC1C,OAAO,UAAU,CAAC,GAAG,CAAC,MAAM,SAAS,EAAE,CAAC;;aACnC;IACL,QAAA,MAAM,WAAW,GAAG,KAAK,CAAC,CAAC,CAAC;YAC5B,MAAM,UAAU,GAAG,KAAK,CAAC,WAAW,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC;IAC7C,QAAA,OAAO,UAAU,CAAC,GAAG,CAAC,MAAM,kBAAkB,CAAC,KAAK,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,SAAS,CAAC,CAAC;;IAE9E;IAEA;;;;;;IAMG;IACG,SAAU,MAAM,CACpB,IAAgB,EAChB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;QAEd,OAAO,IAAI,MAAM,CAAC,IAAI,EAAE,aAAa,EAAE,MAAM,CAAC;IAChD;IAEA;;;;;;IAMG;IACG,SAAU,KAAK,CACnB,KAAoB,EACpB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;IAEd,IAAA,OAAO,IAAI,MAAM,CACf,kBAAkB,CAAC,KAAK,EAAE,MAAM,CAAC,CAAC,EAClC,aAAa,EACb,MAAM,CACP;IACH;IAEA;;;;;;IAMG;IACG,SAAU,IAAI,CAClB,KAAoB,EACpB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;IAEd,IAAA,OAAO,IAAI,MAAM,CACf,kBAAkB,CAAC,KAAK,EAAE,MAAM,CAAC,CAAC,EAClC,aAAa,EACb,MAAM,CACP;IACH;IAEA;;;;;;IAMG;IACG,SAAU,IAAI,CAClB,KAAoB,EACpB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;QAEd,MAAM,CAAC,GAAG,IAAI,CAAC,KAAK,EAAE,aAAa,CAAC;IACpC,IAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE;IACjC,QAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE;IACjC,YAAA,IAAI,CAAC,GAAG,CAAC,EAAE;oBACT,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC;;;;QAKvB,OAAO,IAAI,MAAM,CAAC,CAAC,CAAC,KAAK,EAAE,aAAa,EAAE,MAAM,CAAC;IACnD;IAEA;;;;;;IAMG;IACG,SAAU,IAAI,CAClB,KAAoB,EACpB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EAAA;QAEd,OAAO,IAAI,MAAM,CACf,kBAAkB,CAAC,KAAK,EAAE,MAAM,IAAI,CAAC,MAAM,EAAE,CAAC,EAC9C,aAAa,EACb,MAAM,CACP;IACH;IAEA;;;;;;;IAOG;IACa,SAAA,KAAK,CACnB,KAAoB,EACpB,aAAa,GAAG,KAAK,EACrB,MAAM,GAAG,KAAK,EACd,MAAM,GAAG,KAAK,EAAA;QAEd,OAAO,IAAI,MAAM,CACf,kBAAkB,CAAC,KAAK,EAAE,MAAK;YAC7B,MAAM,IAAI,GAAG,IAAI,CAAC,MAAM,EAAE,GAAG,IAAI,GAAG,IAAI;YACxC,MAAM,QAAQ,GAAG,IAAI,CAAC,MAAM,EAAE,GAAG,IAAI,GAAG,IAAI;IAC5C,QAAA,MAAM,GAAG,GACP,IAAI,CAAC,IAAI,CAAC,CAAC,GAAG,GAAG,IAAI,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,GAAG,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,IAAI,CAAC,EAAE,GAAG,QAAQ,CAAC;YACvE,IAAI,MAAM,EAAE;;gBAEV,OAAO,GAAG,GAAG,IAAI,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC;;iBAC3B;IACL,YAAA,OAAO,GAAG;;IAEd,KAAC,CAAC,EACF,aAAa,EACb,MAAM,CACP;IACH;IAEA;;;;;;;IAOG;aACa,OAAO,CACrB,GAAG,GAAG,CAAC,EACP,IAAI,GAAG,CAAC,EACR,KAAK,GAAG,CAAC,CAAC,CAAC,EACX,aAAa,GAAG,KAAK,EAAA;QAErB,OAAO,IAAI,MAAM,CACf,kBAAkB,CAAC,KAAK,EAAE,MAAK;IAC7B,QAAA,OAAO,IAAI,CAAC,KAAK,CAAC,IAAI,CAAC,MAAM,EAAE,IAAI,IAAI,GAAG,GAAG,CAAC,CAAC,GAAG,GAAG;IACvD,KAAC,CAAC,EACF,aAAa,CACd;IACH;IAEA;IAEA;;;;IAIG;IACG,SAAU,YAAY,CAAC,CAA+B,EAAA;IAC1D,IAAA,IAAI,CAAC,YAAY,MAAM,EAAE;YACvB,OAAO,CAAC,CAAC,aAAa;;aACjB;IACL,QAAA,OAAO,KAAK;;IAEhB;IAEA;;;;;;;;;;;;IAYG;IACa,SAAA,SAAS,CAAC,CAAS,EAAE,CAAS,EAAA;IAC5C,IAAA,SAAS,UAAU,CACjB,GAAwB,EACxB,CAAsB,EAAA;YAEtB,IAAI,OAAO,GAAG,KAAK,QAAQ,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;IACpD,YAAA,OAAO,GAAG;;iBACL,IAAI,OAAO,GAAG,KAAK,QAAQ,IAAI,CAAC,YAAY,KAAK,EAAE;IACxD,YAAA,MAAM,QAAQ,GAAG,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,GAAG,CAAC;IAC1C,YAAA,OAAO,UAAU,CAAC,QAAQ,EAAE,CAAC,CAAC;;iBACzB,IAAI,GAAG,YAAY,KAAK,IAAI,OAAO,CAAC,KAAK,QAAQ,EAAE;gBACxD,OAAO,UAAU,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,CAAC,EAAE,CAAC,CAAC;;iBAC7B,IAAI,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC,KAAK,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,CAAC,CAAC,CAAC,EAAE;IACxE,YAAA,OAAO,GAAG;;iBACL,IAAI,GAAG,YAAY,KAAK,IAAI,CAAC,YAAY,KAAK,EAAE;;IAErD,YAAA,MAAM,QAAQ,GAAG,QAAQ,CAAC,GAAG,CAAC;IAC9B,YAAA,MAAM,MAAM,GAAG,QAAQ,CAAC,CAAC,CAAC;;gBAE1B,IAAI,QAAQ,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IACnC,gBAAA,IAAI,GAAY;;IAEhB,gBAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,QAAQ,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACxC,oBAAA,IACE,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,CAAC,CAAC;IACpD,wBAAA,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,EACtB;4BACA,GAAG,GAAG,CAAC;;;;IAIX,gBAAA,IAAI,GAAG,KAAK,CAAC,EAAE;wBACb,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAAK,UAAU,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;;;yBAExD;;IAEL,oBAAA,OAAO,IAAI,CAAC,GAAG,EAAE,CAAC,CAAC;;;;qBAGhB,IAAI,QAAQ,CAAC,MAAM,GAAG,MAAM,CAAC,MAAM,EAAE;IAC1C,gBAAA,IAAI,GAAY;;IAEhB,gBAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IACtC,oBAAA,IACE,IAAI,CAAC,SAAS,CAAC,MAAM,CAAC,KAAK,CAAC,CAAC,EAAE,CAAC,GAAG,QAAQ,CAAC,MAAM,CAAC,CAAC;IACpD,wBAAA,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,EACxB;4BACA,GAAG,GAAG,CAAC;;;;IAIX,gBAAA,IAAI,GAAG,KAAK,CAAC,EAAE;IACb,oBAAA,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,OAAO,KAAK,UAAU,CAAC,OAAO,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;;yBAEjD;IACL,oBAAA,OAAO,KAAK,CAAC,CAAC,CAAC,MAAM;6BAClB,IAAI,CAAC,CAAC;IACN,yBAAA,GAAG,CAAC,MAAM,UAAU,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;;;qBAEhC;;IAEL,gBAAA,MAAM,kBAAkB,GAAG,CACzB,GAA+B,EAC/B,CAAa,KACC;IACd,oBAAA,IAAI,GAAG,YAAY,KAAK,IAAI,CAAC,CAAC,MAAM,IAAI,GAAG,CAAC,MAAM,EAAE;IAClD,wBAAA,IAAI,CAAC,CAAC,MAAM,KAAK,CAAC,EAAE;;gCAElB,OAAO,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC;;IAChB,6BAAA,IAAI,GAAG,CAAC,MAAM,KAAK,CAAC,EAAE;;IAE3B,4BAAA,MAAM,UAAU,GAAG,KAAK,CAAC,CAAC,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;IAC9C,4BAAA,OAAO,UAAU,CAAC,GAAG,CAAC,MAAM,GAAG,CAAC,CAAC,CAAC,CAAC;;iCAC9B;IACL,4BAAA,MAAM,KAAK,CACT,CAAU,OAAA,EAAA,QAAQ,CAAC,GAAG,CAAC,CAAQ,KAAA,EAAA,QAAQ,CAAC,CAAC,CAAC,CAAA,mBAAA,CAAqB,CAChE;;;6BAEE;;IAEL,wBAAA,IAAI,GAAG,YAAY,KAAK,EAAE;;gCAExB,OAAO,GAAG,CAAC,GAAG,CAAC,CAAC,OAAmB,EAAE,GAAW,KAC9C,kBAAkB,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CACpC;;IACI,6BAAA,IAAI,OAAO,GAAG,KAAK,QAAQ,EAAE;;gCAElC,OAAO,CAAC,IAAI,CAAC,CAAC,GAAG,CAAC,CAAC,OAAO,EAAE,GAAG,KAC7B,kBAAkB,CAAC,OAAO,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,CACpC;;iCACI;IACL,4BAAA,MAAM,KAAK,CAAC,2BAA2B,CAAC;;;IAG9C,iBAAC;;IAED,gBAAA,OAAO,kBAAkB,CAAC,GAAG,EAAE,CAAC,CAAC;;;iBAE9B;IACL,YAAA,MAAM,KAAK,CAAC,2BAA2B,CAAC;;;IAI5C,IAAA,IAAI,GAAG,GAAG,CAAC,CAAC,IAAI;IAChB,IAAA,OAAO,IAAI,CAAC,SAAS,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAC,IAAI,IAAI,CAAC,SAAS,CAAC,CAAC,CAAC,KAAK,CAAC,EAAE;IAC/D,QAAA,GAAG,GAAG,WAAW,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,CAAC,IAAI,CAAC,CAAC;;IAE5C,IAAA,OAAO,IAAI,MAAM,CAAC,GAAG,CAAC;IACxB;IAEA;;;;;;;;IAQG;IACa,SAAA,WAAW,CACzB,SAAqB,EACrB,UAAsB,EAAA;IAEtB,IAAA,SAAS,YAAY,CACnB,SAAqB,EACrB,UAAsB,EAAA;IAEtB,QAAA,IAAI,QAAQ,CAAC,SAAS,CAAC,CAAC,MAAM,GAAG,CAAC,KAAK,QAAQ,CAAC,UAAU,CAAC,CAAC,MAAM,EAAE;;IAElE,YAAA,MAAM,UAAU,GAAG,KAAK,CAAC,UAAU,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;gBACvD,OAAO,UAAU,CAAC,GAAG,CAAC,MAAM,SAAS,CAAC;;iBACjC;;IAEL,YAAA,MAAM,UAAU,GAAG,KAAK,CAAC,UAAU,CAAC,MAAM,CAAC,CAAC,IAAI,CAAC,KAAK,CAAC;gBACvD,OAAO,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,EAAE,GAAG,KAC3B,YAAY,CAAC,SAAS,EAAE,UAAU,CAAC,GAAG,CAAC,CAAC,CACzC;;;IAGL,IAAA,OAAO,QAAQ,CAAC,SAAS,CAAC,CAAC,MAAM,GAAG,QAAQ,CAAC,UAAU,CAAC,CAAC,MAAM,EAAE;IAC/D,QAAA,SAAS,GAAG,YAAY,CAAC,SAAS,EAAE,UAAU,CAAC;;IAEjD,IAAA,OAAO,SAAS;IAClB;;ICvqEA;UACa,MAAM,CAAA;;QAIjB,IAAI,GAAqB,OAAO;IAEhC;;;IAGG;QACH,UAAU,GAAA;;YAER,IAAI,MAAM,GAA2B,EAAE;IACvC,QAAA,KAAK,MAAM,CAAC,CAAC,EAAE,KAAK,CAAC,IAAI,IAAI,CAAC,OAAO,EAAE,EAAE;;IAEvC,YAAA,IAAI,KAAK,YAAY,MAAM,EAAE;oBAC3B,MAAM,GAAG,MAAM,CAAC,MAAM,CAAC,KAAK,CAAC,UAAU,EAAE,CAAC;;IACrC,iBAAA,IAAI,KAAK,YAAY,SAAS,EAAE;IACrC,gBAAA,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC;;IACb,iBAAA,IAAI,KAAK,YAAY,MAAM,EAAE;IAClC,gBAAA,IAAI,KAAK,CAAC,aAAa,EAAE;IACvB,oBAAA,MAAM,CAAC,IAAI,CAAC,KAAK,CAAC;;;;IAIxB,QAAA,OAAO,MAAM;;IAGf;;IAEG;QACH,KAAK,GAAA;IACH,QAAA,IAAI,CAAC,IAAI,GAAG,OAAO;IACnB,QAAA,KAAK,MAAM,CAAC,CAAC,EAAE,KAAK,CAAC,IAAI,IAAI,CAAC,OAAO,EAAE,EAAE;IACvC,YAAA,IAAI,KAAK,YAAY,MAAM,EAAE;oBAC3B,KAAK,CAAC,KAAK,EAAE;;;;IAKnB;;IAEG;QACH,IAAI,GAAA;IACF,QAAA,IAAI,CAAC,IAAI,GAAG,MAAM;IAClB,QAAA,KAAK,MAAM,CAAC,CAAC,EAAE,KAAK,CAAC,IAAI,IAAI,CAAC,OAAO,EAAE,EAAE;IACvC,YAAA,IAAI,KAAK,YAAY,MAAM,EAAE;oBAC3B,KAAK,CAAC,IAAI,EAAE;;;;IAKlB;;;IAGG;QACH,OAAO,GAAA;IACL,QAAA,OAAO,MAAM,CAAC,OAAO,CAAC,IAAI,CAAC;;IAE9B;IAED;IAEM,MAAO,MAAO,SAAQ,MAAM,CAAA;IACzB,IAAA,CAAC;IACD,IAAA,CAAC;IACD,IAAA,QAAQ;IACf;;;;;;;;IAQG;IACH,IAAA,WAAA,CACE,OAAe,EACf,QAAgB,EAChB,MAAM,GAAG,KAAK,EACd,IAAI,GAAG,IAAI,EACX,MAAM,GAAG,IAAI,EAAA;IAEb,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,OAAO,EAAE,QAAQ,CAAC,EAAE,IAAI,EAAE,MAAM,EAAE,MAAM,CAAC;YACzD,IAAI,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,QAAQ,CAAC,EAAE,IAAI,CAAC;IAChC,QAAA,IAAI,CAAC,QAAQ,GAAG,IAAI;;IAGtB;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;YACf,IAAI,CAAC,GAAG,CAAC,CAAC,MAAM,CAAC,IAAI,CAAC,CAAC,CAAC;IACxB,QAAA,IAAI,IAAI,CAAC,QAAQ,EAAE;gBACjB,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,CAAC,CAAC;;IAEnB,QAAA,OAAO,CAAC,GAAG,CAAC,aAAa,CAAC;IAC1B,QAAA,OAAO,CAAC;;IAEX;IAEK,MAAO,sBAAuB,SAAQ,MAAM,CAAA;IACzC,IAAA,EAAE;IACF,IAAA,EAAE;IACF,IAAA,EAAE;IACF,IAAA,aAAa;IACb,IAAA,IAAI;IACJ,IAAA,WAAW;IACX,IAAA,gBAAgB;IAChB,IAAA,OAAO;IACP,IAAA,CAAC;IAER;;;;;;;;;IASG;IACH,IAAA,WAAA,CACE,OAAe,EACf,QAAgB,EAChB,OAAe,EACf,WAAmB,EACnB,YAAY,GAAG,CAAC,EAChB,MAAM,GAAG,KAAK,EAAA;IAEd,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,EAAE,OAAO,EAAE,MAAM,EAAE,IAAI,EAAE,KAAK,CAAC;IAC3D,QAAA,IAAI,CAAC,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,EAAE,OAAO,EAAE,MAAM,EAAE,IAAI,EAAE,KAAK,CAAC;IAC3D,QAAA,IAAI,CAAC,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,EAAE,OAAO,EAAE,MAAM,EAAE,IAAI,EAAE,KAAK,CAAC;IAC3D,QAAA,IAAI,CAAC,aAAa,GAAG,IAAI,MAAM,CAAC,OAAO,EAAE,QAAQ,EAAE,MAAM,EAAE,IAAI,EAAE,KAAK,CAAC;IACvE,QAAA,IAAI,CAAC,IAAI,GAAG,IAAI,CAAC,CAAC,WAAW,EAAE,WAAW,CAAC,EAAE,KAAK,CAAC;YACnD,IAAI,CAAC,WAAW,GAAG,IAAI,OAAO,CAAC,YAAY,CAAC;YAC5C,IAAI,CAAC,gBAAgB,GAAG,IAAI,OAAO,CAAC,YAAY,CAAC;IACjD,QAAA,IAAI,CAAC,OAAO,GAAG,IAAI,OAAO,EAAE;;IAG5B,QAAA,IAAI,CAAC,CAAC,GAAG,OAAO,GAAG,OAAO;IAC1B,QAAA,IAAI,OAAO,GAAG,OAAO,IAAI,CAAC,EAAE;IAC1B,YAAA,MAAM,IAAI,KAAK,CAAC,mDAAmD,CAAC;;;IAIxE;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;YACf,MAAM,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,GAAG,CAAC,CAAC,KAAK;IACzB,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,CAAC;IAChB,QAAA,MAAM,EAAE,GAAG,CAAC,GAAG,CAAC,CAAC;;IAGjB,QAAA,IAAI,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3B,QAAA,IAAI,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IAC3B,QAAA,IAAI,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;;YAG3B,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAC7C,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAC7C,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,EAAE,EAAE,CAAC,CAAC,CAAC,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;;IAG7C,QAAA,MAAM,EAAE,GAAG,CAAC,CAAC,SAAS,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YAC9B,IAAI,GAAG,GAAG,CAAC,CAAC,MAAM,CAAC,EAAE,CAAC,CAAC;;YAGvB,GAAG,GAAG,GAAG,CAAC,GAAG,CAAC,CAAC,IAAI,CAAC,CAAC;;YAGrB,MAAM,IAAI,GAAG,SAAS,CAAC,IAAI,CAAC,IAAI,EAAE,GAAG,CAAC;IACtC,QAAA,GAAG,GAAG,GAAG,CAAC,WAAW,CAAC,IAAI,EAAE,CAAC,EAAU,KAAc,EAAE,KAAK,CAAC,EAAE,CAAC,QAAQ,CAAC;IACzE,QAAA,GAAG,GAAG,IAAI,CAAC,OAAO,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,CAAC,CAAC;YACnC,GAAG,GAAG,IAAI,CAAC,WAAW,CAAC,OAAO,CAAC,GAAG,CAAC;;YAGnC,IAAI,GAAG,GAAG,GAAG,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;;YAGxB,GAAG,GAAG,GAAG,CAAC,SAAS,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC;;YAG7C,GAAG,GAAG,IAAI,CAAC,aAAa,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC;YACtC,GAAG,GAAG,IAAI,CAAC,gBAAgB,CAAC,OAAO,CAAC,GAAG,CAAC;IAExC,QAAA,OAAO,GAAG;;IAEb;IAEK,MAAO,cAAe,SAAQ,MAAM,CAAA;IACjC,IAAA,EAAE;IACF,IAAA,IAAI;IACJ,IAAA,EAAE;IACF,IAAA,OAAO;IACd;;;;;;;;IAQG;IACH,IAAA,WAAA,CACE,OAAe,EACf,QAAgB,EAChB,YAAY,GAAG,CAAC,EAChB,MAAiB,GAAA,KAAK,EACtB,IAAA,GAAgB,IAAI,EAAA;IAEpB,QAAA,KAAK,EAAE;IAEP,QAAA,IAAI,CAAC,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,EAAE,OAAO,GAAG,CAAC,EAAE,MAAM,EAAE,IAAI,EAAE,IAAI,CAAC;IAC9D,QAAA,IAAI,CAAC,IAAI,GAAG,IAAI,IAAI,EAAE;IACtB,QAAA,IAAI,CAAC,EAAE,GAAG,IAAI,MAAM,CAAC,OAAO,GAAG,CAAC,EAAE,QAAQ,CAAC;YAC3C,IAAI,CAAC,OAAO,GAAG,IAAI,OAAO,CAAC,YAAY,CAAC;;IAG1C;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;YACf,IAAI,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC;YAC1B,CAAC,GAAG,IAAI,CAAC,IAAI,CAAC,OAAO,CAAC,CAAC,CAAC;YACxB,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,OAAO,CAAC,CAAC,CAAC;YACtB,CAAC,GAAG,IAAI,CAAC,OAAO,CAAC,OAAO,CAAC,CAAC,CAAC;IAC3B,QAAA,OAAO,CAAC;;IAEX;IAEK,MAAO,KAAM,SAAQ,MAAM,CAAA;IACxB,IAAA,GAAG;IACH,IAAA,GAAG;IACH,IAAA,GAAG;IACH,IAAA,GAAG;IAEV;;;;;;;;;IASG;IACH,IAAA,WAAA,CACE,OAAe,EACf,QAAgB,EAChB,OAAe,EACf,WAAmB,EACnB,YAAY,GAAG,CAAC,EAChB,MAAM,GAAG,KAAK,EAAA;IAEd,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,GAAG,GAAG,IAAI,sBAAsB,CACnC,OAAO,EACP,OAAO,EACP,OAAO,EACP,WAAW,EACX,YAAY,EACZ,MAAM,CACP;YACD,IAAI,CAAC,GAAG,GAAG,IAAI,SAAS,CAAC,OAAO,CAAC;IACjC,QAAA,IAAI,CAAC,GAAG,GAAG,IAAI,cAAc,CAC3B,OAAO,EACP,QAAQ,EACR,YAAY,EACZ,MAAM,EACN,IAAI,CACL;YACD,IAAI,CAAC,GAAG,GAAG,IAAI,SAAS,CAAC,QAAQ,CAAC;;IAGpC;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;YACf,IAAI,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;;YAEpD,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC,OAAO,CAAC,IAAI,CAAC,GAAG,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC,CAAC;;IAEhD,QAAA,OAAO,CAAC;;IAEX;IAED;IAEM,MAAO,SAAU,SAAQ,MAAM,CAAA;IAC5B,IAAA,CAAC;IAER;;;;;IAKG;QACH,WAAY,CAAA,UAAkB,EAAE,UAAkB,EAAA;IAChD,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,UAAU,EAAE,UAAU,CAAC,EAAE,IAAI,EAAE,KAAK,EAAE,KAAK,CAAC;;IAG9D;;;;IAIG;IACH,IAAA,OAAO,CAAC,GAAW,EAAA;;YAEjB,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,GAAG,CAAC,KAAK;YAExB,IAAI,CAAC,GAAG,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,GAAG,CAAC;;YAGtB,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,CAAC,CAAC;IAEtC,QAAA,OAAO,CAAC;;IAEX;IAEK,MAAO,mBAAoB,SAAQ,MAAM,CAAA;IACtC,IAAA,CAAC;IAER;;;;;IAKG;QACH,WAAY,CAAA,UAAkB,EAAE,UAAkB,EAAA;IAChD,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,UAAU,EAAE,UAAU,CAAC,EAAE,IAAI,EAAE,KAAK,EAAE,KAAK,CAAC;;IAG9D;;;;IAIG;IACH,IAAA,OAAO,CAAC,GAAW,EAAA;;YAEjB,MAAM,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,GAAG,CAAC,KAAK;;IAExB,QAAA,MAAM,CAAC,GAAG,IAAI,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC;IAEzC,QAAA,OAAO,CAAC;;IAEX;IAED;IAEM,MAAO,IAAK,SAAQ,MAAM,CAAA;IAC9B;;IAEG;IACH,IAAA,WAAA,GAAA;IACE,QAAA,KAAK,EAAE;;IAGT;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;;YAEf,SAAS,KAAK,CAAC,CAAa,EAAA;;gBAE1B,IAAI,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;IAC5B,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,EAAU,KAAY;IAClC,oBAAA,IAAI,EAAE,GAAG,CAAC,EAAE;IACV,wBAAA,OAAO,GAAG;;6BACL;IACL,wBAAA,OAAO,KAAK;;IAEhB,iBAAC,CAAC;;;qBAEG,IAAI,OAAO,CAAC,CAAC,CAAC,CAAC,KAAK,QAAQ,EAAE;IACnC,gBAAA,OAAO,CAAC,CAAC,GAAG,CAAC,CAAC,EAAc,KAAiB,KAAK,CAAC,EAAE,CAAC,CAAC;;;IAClD,gBAAA,MAAM,KAAK,CAAC,6CAA6C,CAAC;;YAEnE,MAAM,IAAI,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC;IAEnC,QAAA,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC;IACf,QAAA,OAAO,CAAC;;IAEX;IAEK,MAAO,OAAQ,SAAQ,MAAM,CAAA;IACjC;;IAEG;IACH,IAAA,WAAA,GAAA;IACE,QAAA,KAAK,EAAE;;IAGT;;;;;IAKG;IACH,IAAA,OAAO,CAAC,CAAS,EAAE,GAAG,GAAG,CAAC,CAAC,EAAA;IACzB,QAAA,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC;IACV,QAAA,MAAM,GAAG,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,CAAC,GAAG,EAAE,IAAI,CAAC,CAAC;IACnC,QAAA,OAAO,GAAG;;IAEb;IAED;IAEM,MAAO,OAAQ,SAAQ,MAAM,CAAA;IAC1B,IAAA,CAAC;IAER;;;;IAIG;IACH,IAAA,WAAA,CAAY,SAAiB,EAAA;IAC3B,QAAA,KAAK,EAAE;IACP,QAAA,IAAI,CAAC,CAAC,GAAG,SAAS;IAClB,QAAA,IAAI,CAAC,IAAI,GAAG,OAAO;;IAErB;;;;IAIG;IACH,IAAA,OAAO,CAAC,CAAS,EAAA;IACf,QAAA,IAAI,IAAI,CAAC,IAAI,IAAI,MAAM,EAAE;IACvB,YAAA,OAAO,CAAC;;YAEV,MAAM,IAAI,GAAG,IAAI,CAAC,CAAC,CAAC,KAAK,CAAC;;YAE1B,IAAI,CAAC,GAAG,CAAC,CAAC,WAAW,CACnB,IAAI,EACJ,CAAC,EAAU,KAAa;IACtB,YAAA,OAAO,EAAE,GAAG,IAAI,CAAC,CAAC;aACnB,EACD,CAAC,CACF;;YAED,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,CAAC,CAAC;IACrB,QAAA,OAAO,CAAC;;IAEX;IAEK,MAAO,SAAU,SAAQ,MAAM,CAAA;IAC5B,IAAA,KAAK;IACL,IAAA,IAAI;IAEX;;;;IAIG;IACH,IAAA,WAAA,CAAY,OAAe,EAAA;IACzB,QAAA,KAAK,EAAE;YACP,IAAI,CAAC,KAAK,GAAG,IAAI,CAAC,CAAC,OAAO,CAAC,EAAE,IAAI,CAAC;YAClC,IAAI,CAAC,IAAI,GAAG,KAAK,CAAC,CAAC,OAAO,CAAC,EAAE,IAAI,CAAC;;IAGpC,IAAA,OAAO,CAAC,CAAS,EAAA;IACf,QAAA,MAAM,KAAK,GAAG,CAAC,CAAC,QAAQ,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC;YACnC,MAAM,MAAM,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC,IAAI,CAAC,CAAC,CAAC,EAAE,IAAI,CAAC,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,KAAK,CAAC,CAAC,CAAC;IACxD,QAAA,MAAM,CAAC,GAAG,GAAG,CAAC,MAAM,EAAE,IAAI,CAAC,KAAK,CAAC,CAAC,GAAG,CAAC,IAAI,CAAC,IAAI,CAAC,CAAC;IACjD,QAAA,OAAO,CAAC;;IAEX;IAED;IAEM,MAAO,gBAAiB,SAAQ,MAAM,CAAA;IAC1C;;IAEG;IACH,IAAA,WAAA,GAAA;IACE,QAAA,KAAK,EAAE;;IAGT;;;;;IAKG;QACH,OAAO,CAAC,CAAS,EAAE,CAAS,EAAA;;IAE1B,QAAA,IAAI,KAAK,GAAG,CAAC,CAAC,KAAK;;IAEnB,QAAA,MAAM,CAAC,GAAG,KAAK,CAAC,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;;IAExD,QAAA,KAAK,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;IACxC,QAAA,MAAM,CAAC,GAAG,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;;YAE1C,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;;IAGrB,QAAA,MAAM,SAAS,GAAG,GAAG,CAAC,CAAC,CAAC;YAExB,MAAM,SAAS,GAAG,SAAS,CAAC,GAAG,CAAC,CAAC,EAAE,IAAI,CAAC;YAExC,MAAM,MAAM,GAAG,SAAS,CAAC,GAAG,CAAC,SAAS,CAAC;IAEvC,QAAA,MAAM,OAAO,GAAG,QAAQ,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,CAAC,CAAC,CAAC;;IAGrC,QAAA,MAAM,SAAS,GAAG,MAAM,CAAC,EAAE,CAAC,CAAC,GAAG,KAAK,CAAC,CAAC,CAAC,CAAC,IAAI,EAAE,CAAC,EAAE,OAAO,CAAC;IAC1D,QAAA,MAAM,UAAU,GAAG,GAAG,CAAC,SAAS,CAAC;IACjC,QAAA,IAAI,IAAI,GAAG,UAAU,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,EAAE;IACnC,QAAA,IAAI,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;IAClB,QAAA,OAAO,IAAI;;IAEd;IACD;;IAEG;IACG,MAAO,OAAQ,SAAQ,MAAM,CAAA;IACjC;;IAEG;IACH,IAAA,WAAA,GAAA;IACE,QAAA,KAAK,EAAE;;IAGT;;;;;IAKG;QACH,OAAO,CAAC,CAAS,EAAE,CAAS,EAAA;;IAE1B,QAAA,IAAI,KAAK,GAAG,CAAC,CAAC,KAAK;;IAEnB,QAAA,MAAM,CAAC,GAAG,KAAK,CAAC,KAAK,CAAC,KAAK,CAAC,MAAM,GAAG,CAAC,EAAE,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;;IAExD,QAAA,KAAK,GAAG,KAAK,CAAC,KAAK,CAAC,CAAC,EAAE,KAAK,CAAC,MAAM,GAAG,CAAC,CAAC;IACxC,QAAA,MAAM,CAAC,GAAG,KAAK,CAAC,MAAM,CAAC,CAAC,CAAC,EAAE,CAAC,KAAK,CAAC,GAAG,CAAC,EAAE,CAAC,CAAC;;YAE1C,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrB,CAAC,GAAG,CAAC,CAAC,OAAO,CAAC,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC;YACrB,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;YAClB,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,CAAC,CAAC,CAAC;IAClB,QAAA,MAAM,EAAE,GAAG,CAAC,CAAC,GAAG,EAAE;IAClB,QAAA,IAAI,IAAI,GAAG,EAAE,CAAC,IAAI,EAAE;IACpB,QAAA,IAAI,GAAG,IAAI,CAAC,GAAG,CAAC,CAAC,CAAC;IAClB,QAAA,OAAO,IAAI;;IAEd;IAED;;;;IAIG;IACa,SAAA,IAAI,CAAC,KAAa,EAAE,IAAY,EAAA;IAC9C;;;;IAIG;QACH,SAAS,iBAAiB,CAAC,GAA2B,EAAA;YAGpD,IAAI,MAAM,GAA2B,EAAE;IACvC,QAAA,KAAK,IAAI,CAAC,IAAI,GAAG,EAAE;gBACjB,IACE,CAAC,KAAK,eAAe;IACrB,gBAAA,CAAC,KAAK,iBAAiB;IACvB,gBAAA,CAAC,KAAK,iBAAiB;oBACvB,CAAC,KAAK,KAAK,EACX;IACA,gBAAA,IAAI,OAAO,GAAG,CAAC,CAAC,CAAC,KAAK,QAAQ,IAAI,CAAC,KAAK,CAAC,OAAO,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,EAAE;wBACxD,MAAM,CAAC,CAAC,CAAC,GAAG,iBAAiB,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC;;yBAChC;wBACL,MAAM,CAAC,CAAC,CAAC,GAAG,GAAG,CAAC,CAAC,CAAC;;;qBAEf;IACL,gBAAA,MAAM,CAAC,CAAC,CAAC,GAAG,IAAI;;;IAGpB,QAAA,OAAO,MAAM;;QAEf,MAAM,IAAI,GAAG,IAAI,CAAC,SAAS,CAAC,iBAAiB,CAAC,KAAK,CAAC,CAAC;IACrD,IAAA,EAAE,CAAC,aAAa,CAAC,IAAI,EAAE,IAAI,CAAC;IAC9B;IAEA;;;;;IAKG;IACa,SAAA,IAAI,CAAC,KAAa,EAAE,IAAY,EAAA;QAC9C,MAAM,UAAU,GAAG,EAAE,CAAC,YAAY,CAAC,IAAI,CAAC;QACxC,IAAI,WAAW,GAAG,IAAI,CAAC,KAAK,CAAC,UAAU,CAAC,QAAQ,EAAE,CAAC;IACnD,IAAA,cAAc,CAAC,WAAW,EAAE,KAAK,CAAC;IAClC,IAAA,OAAO,KAAK;IACd;IAEA,SAAS,cAAc,CAAC,MAAc,EAAE,MAAc,EAAA;IACpD,IAAA,KAAK,MAAM,CAAC,GAAG,EAAE,KAAK,CAAC,IAAI,MAAM,CAAC,OAAO,EAAE,EAAE;;IAE3C,QAAA,IAAI,KAAK,YAAY,MAAM,EAAE;gBAC3B,cAAc,CAAC,MAAM,CAAC,GAAG,CAAC,EAAE,MAAM,CAAC,GAAG,CAAC,CAAC;;iBACnC,IAAI,KAAK,YAAY,SAAS,IAAI,KAAK,YAAY,MAAM,EAAE;IAChE,YAAA,MAAM,CAAC,GAAG,CAAC,CAAC,KAAK,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,KAAK;IACrC,YAAA,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;IAC7B,YAAA,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC,GAAG,MAAM,CAAC,GAAG,CAAC,CAAC,CAAC;;;IAGnC;;UC7oBa,IAAI,CAAA;;IAEf,IAAA,MAAM;IACN,IAAA,EAAE;IACF,IAAA,GAAG;IACH,IAAA,EAAE;IACF,IAAA,EAAE;IACF,IAAA,GAAG;IAEH;;;;;;;IAOG;QACH,WACE,CAAA,MAA8B,EAC9B,EAAE,GAAG,IAAI,EACT,GAAG,GAAG,CAAC,EACP,KAAK,GAAG,CAAC,GAAG,EAAE,IAAI,CAAC,EACnB,GAAG,GAAG,IAAI,EAAA;IAEV,QAAA,IAAI,CAAC,MAAM,GAAG,MAAM;IACpB,QAAA,IAAI,CAAC,EAAE,GAAG,EAAE;IACZ,QAAA,IAAI,CAAC,GAAG,GAAG,GAAG;IACd,QAAA,IAAI,CAAC,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC;IAClB,QAAA,IAAI,CAAC,EAAE,GAAG,KAAK,CAAC,CAAC,CAAC;IAClB,QAAA,IAAI,CAAC,GAAG,GAAG,GAAG;IACd,QAAA,IAAI,CAAC,GAAG,GAAG,GAAG;;IAEd,QAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IAC3C,YAAA,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,KAAK,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;IAC9C,YAAA,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,KAAK,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,CAAC;;;IAIlD;;IAEG;QACH,IAAI,GAAA;IACF,QAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;IAC3C,YAAA,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;IAChC,kBAAE,GAAG,CAAC,IAAI,CAAC,EAAE;IACZ,iBAAA,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,CAAC;IAC9C,YAAA,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;IAChC,kBAAE,GAAG,CAAC,IAAI,CAAC,EAAE;qBACZ,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,EAAE,GAAG,CAAC,CAAC,CAAC,CAAC,GAAG,CAAC,CAAC,GAAG,IAAI,CAAC,EAAE,CAAC,CAAC;gBAErD,MAAM,aAAa,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC;IACnC,kBAAE,GAAG,CAAC,IAAI,CAAC,EAAE;IACZ,iBAAA,GAAG,CAAC,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,CAAC,EAAE,IAAI,EAAE,CAAC,GAAG,CAAC,IAAI,CAAC,GAAG,CAAC;IAC1C,iBAAA,GAAG,EAAE;IACR,YAAA,MAAM,qBAAqB,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC;qBACxC,GAAG,CAAC,IAAI,CAAC,GAAG,GAAG,IAAI,CAAC,EAAE;IACtB,iBAAA,GAAG,EAAE;gBAER,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,KAAK,GAAG,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,GAAG,CACvC,aAAa,EAAE,GAAG,CAAC,qBAAqB,CAAC,CAC1C,CAAC,KAAK;;;IAIX;;IAEG;QACH,SAAS,GAAA;IACP,QAAA,KAAK,IAAI,CAAC,GAAG,CAAC,EAAE,CAAC,GAAG,IAAI,CAAC,MAAM,CAAC,MAAM,EAAE,CAAC,EAAE,EAAE;gBAC3C,IAAI,CAAC,MAAM,CAAC,CAAC,CAAC,CAAC,SAAS,EAAE;;;IAG/B;;IC1BD,MAAM,EAAE,GAAG;QACT,MAAM;QACN,MAAM;QACN,sBAAsB;QACtB,cAAc;QACd,KAAK;QACL,SAAS;QACT,mBAAmB;QACnB,IAAI;QACJ,OAAO;QACP,OAAO;QACP,SAAS;QACT,gBAAgB;QAChB;KACD;IAED,MAAM,KAAK,GAAG,EAAE,IAAI,EAAE;AAET,UAAA,KAAK,GAAG;;QAEnB,MAAM;QACN,SAAS;QACT,GAAG;QACH,GAAG;QACH,GAAG;QACH,GAAG;QACH,MAAM;QACN,GAAG;QACH,GAAG;QACH,IAAI;QACJ,GAAG;QACH,IAAI;QACJ,WAAW;QACX,QAAQ;QACR,EAAE;QACF,OAAO;QACP,QAAQ;QACR,SAAS;QACT,MAAM;QACN,OAAO;QACP,KAAK;QACL,IAAI;QACJ,IAAI;QACJ,IAAI;QACJ,KAAK;QACL,SAAS;QACT,IAAI;QACJ,IAAI;;QAEJ,EAAE;QACF,KAAK;QACL;;;;;;;;;;;"}